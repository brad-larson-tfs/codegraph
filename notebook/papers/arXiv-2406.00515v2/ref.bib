@article{zhang2023unifying,
  title={Unifying the perspectives of nlp and software engineering: A survey on language models for code},
  author={Zhang, Ziyin and Chen, Chaoyu and Liu, Bingchang and Liao, Cong and Gong, Zi and Yu, Hang and Li, Jianguo and Wang, Rui},
  journal={arXiv preprint arXiv:2311.07989},
  year={2023}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{niu2022deep,
  title={Deep learning meets software engineering: A survey on pre-trained models of source code},
  author={Niu, Changan and Li, Chuanyi and Luo, Bin and Ng, Vincent},
  journal={arXiv preprint arXiv:2205.11739},
  year={2022}
}
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}
@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}
@article{zhou2023language,
  title={Language agent tree search unifies reasoning acting and planning in language models},
  author={Zhou, Andy and Yan, Kai and Shlapentokh-Rothman, Michal and Wang, Haohan and Wang, Yu-Xiong},
  journal={arXiv preprint arXiv:2310.04406},
  year={2023}
}
@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{yao2024tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}
@inproceedings{yao2023react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2023}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}
@article{manyika2023overview,
  title={An overview of Bard: an early experiment with generative AI},
  author={Manyika, James and Hsiao, Sissie},
  journal={AI. Google Static Documents},
  volume={2},
  year={2023}
}
@article{wang2023aligning,
  title={Aligning large language models with human: A survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2307.12966},
  year={2023}
}
@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav},
  journal={arXiv preprint arXiv:2309.00267},
  year={2023}
}
@article{ji2023ai,
  title={Ai alignment: A comprehensive survey},
  author={Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and others},
  journal={arXiv preprint arXiv:2310.19852},
  year={2023}
}
@inproceedings{ross2023programmer,
  title={The programmerâ€™s assistant: Conversational interaction with a large language model for software development},
  author={Ross, Steven I and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D},
  booktitle={Proceedings of the 28th International Conference on Intelligent User Interfaces},
  pages={491--514},
  year={2023}
}
@inproceedings{mousavi2024investigation,
  title={An investigation into misuse of java security apis by large language models},
  author={Mousavi, Zahra and Islam, Chadni and Moore, Kristen and Abuadbba, Alsharif and Babar, M Ali},
  booktitle={Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
  pages={1299--1315},
  year={2024}
}
@inproceedings{liesenfeld2023opening,
  title={Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators},
  author={Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
  booktitle={Proceedings of the 5th international conference on conversational user interfaces},
  pages={1--6},
  year={2023}
}
@article{bogina2022educating,
  title={Educating software and AI stakeholders about algorithmic fairness, accountability, transparency and ethics},
  author={Bogina, Veronika and Hartman, Alan and Kuflik, Tsvi and Shulner-Tal, Avital},
  journal={International Journal of Artificial Intelligence in Education},
  pages={1--26},
  year={2022},
  publisher={Springer}
}
@article{mouselinos2022simple,
  title={A simple, yet effective approach to finding biases in code generation},
  author={Mouselinos, Spyridon and Malinowski, Mateusz and Michalewski, Henryk},
  journal={arXiv preprint arXiv:2211.00609},
  year={2022}
}
@article{liu2023uncovering,
  title={Uncovering and quantifying social biases in code generation},
  author={Liu, Yan and Chen, Xiaokang and Gao, Yan and Su, Zhe and Zhang, Fengji and Zan, Daoguang and Lou, Jian-Guang and Chen, Pin-Yu and Ho, Tsung-Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={2368--2380},
  year={2023}
}
@article{guo2020graphcodebert,
  title={Graphcodebert: Pre-training code representations with data flow},
  author={Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  journal={arXiv preprint arXiv:2009.08366},
  year={2020}
}
@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}
@inproceedings{wei2023towards,
  title={Towards greener yet powerful code generation via quantization: An empirical study},
  author={Wei, Xiaokai and Gonugondla, Sujan Kumar and Wang, Shiqi and Ahmad, Wasi and Ray, Baishakhi and Qian, Haifeng and Li, Xiaopeng and Kumar, Varun and Wang, Zijian and Tian, Yuchen and others},
  booktitle={Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={224--236},
  year={2023}
}
@inproceedings{shi2024greening,
  title={Greening large language models of code},
  author={Shi, Jieke and Yang, Zhou and Kang, Hong Jin and Xu, Bowen and He, Junda and Lo, David},
  booktitle={Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Society},
  pages={142--153},
  year={2024}
}
@inproceedings{samsi2023words,
  title={From words to watts: Benchmarking the energy costs of large language model inference},
  author={Samsi, Siddharth and Zhao, Dan and McDonald, Joseph and Li, Baolin and Michaleas, Adam and Jones, Michael and Bergeron, William and Kepner, Jeremy and Tiwari, Devesh and Gadepally, Vijay},
  booktitle={2023 IEEE High Performance Extreme Computing Conference (HPEC)},
  pages={1--9},
  year={2023},
  organization={IEEE}
}
@inproceedings{chien2023reducing,
  title={Reducing the Carbon Impact of Generative AI Inference (today and in 2035)},
  author={Chien, Andrew A and Lin, Liuzixuan and Nguyen, Hai and Rao, Varsha and Sharma, Tristan and Wijayawardana, Rajini},
  booktitle={Proceedings of the 2nd workshop on sustainable computer systems},
  pages={1--7},
  year={2023}
}
@article{yang2024robustness,
  title={Robustness, security, privacy, explainability, efficiency, and usability of large language models for code},
  author={Yang, Zhou and Sun, Zhensu and Yue, Terry Zhuo and Devanbu, Premkumar and Lo, David},
  journal={arXiv preprint arXiv:2403.07506},
  year={2024}
}
@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav},
  journal={arXiv preprint arXiv:2309.00267},
  year={2023}
}
@inproceedings{al2024traces,
  title={Traces of memorisation in large language models for code},
  author={Al-Kaswan, Ali and Izadi, Maliheh and Van Deursen, Arie},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}
@inproceedings{yang2024unveiling,
  title={Unveiling memorization in code models},
  author={Yang, Zhou and Zhao, Zhipeng and Wang, Chenyu and Shi, Jieke and Kim, Dongsun and Han, Donggyun and Lo, David},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}
@inproceedings{hajipour2024codelmsec,
  title={CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models},
  author={Hajipour, Hossein and Hassler, Keno and Holz, Thorsten and Sch{\"o}nherr, Lea and Fritz, Mario},
  booktitle={2024 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  pages={684--709},
  year={2024},
  organization={IEEE}
}
@inproceedings{schuster2021you,
  title={You autocomplete me: Poisoning vulnerabilities in neural code completion},
  author={Schuster, Roei and Song, Congzheng and Tromer, Eran and Shmatikov, Vitaly},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={1559--1575},
  year={2021}
}
@book{tunstall2022natural,
  title={Natural language processing with transformers},
  author={Tunstall, Lewis and Von Werra, Leandro and Wolf, Thomas},
  year={2022},
  publisher={" O'Reilly Media, Inc."}
}
@article{lialin2023scaling,
  title={Scaling down to scale up: A guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023}
}
@article{muennighoff2023octopack,
  title={Octopack: Instruction tuning code large language models},
  author={Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and Von Werra, Leandro and Longpre, Shayne},
  journal={arXiv preprint arXiv:2308.07124},
  year={2023}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@inproceedings{iyer2018mapping,
  title={Mapping Language to Code in Programmatic Context},
  author={Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={1643--1652},
  year={2018}
}
@inproceedings{yin2018learning,
  title={Learning to mine aligned code and natural language pairs from stack overflow},
  author={Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
  booktitle={Proceedings of the 15th international conference on mining software repositories},
  pages={476--486},
  year={2018}
}
@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}
@article{wang2023aligning,
  title={Aligning large language models with human: A survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2307.12966},
  year={2023}
}
@inproceedings{gan2022measuring,
  title={Measuring and Improving Compositional Generalization in Text-to-SQL via Component Alignment},
  author={Gan, Yujian and Chen, Xinyun and Huang, Qiuping and Purver, Matthew},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2022},
  pages={831--843},
  year={2022}
}
@article{athiwaratkun2022multi,
  title={Multi-lingual evaluation of code generation models},
  author={Athiwaratkun, Ben and Gouda, Sanjay Krishna and Wang, Zijian and Li, Xiaopeng and Tian, Yuchen and Tan, Ming and Ahmad, Wasi Uddin and Wang, Shiqi and Sun, Qing and Shang, Mingyue and others},
  journal={arXiv preprint arXiv:2210.14868},
  year={2022}
}
@article{liu2024your,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}
@article{ding2022delta,
  title={Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={arXiv preprint arXiv:2203.06904},
  year={2022}
}
@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}
@article{iyer2022opt,
  title={Opt-iml: Scaling language model instruction meta learning through the lens of generalization},
  author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  journal={arXiv preprint arXiv:2212.12017},
  year={2022}
}
@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}
@inproceedings{sanh2022multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Le Scao, Teven and Raja, Arun and others},
  booktitle={ICLR 2022-Tenth International Conference on Learning Representations},
  year={2022}
}
@article{le2022coderl,
  title={Coderl: Mastering code generation through pretrained models and deep reinforcement learning},
  author={Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven Chu Hong},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21314--21328},
  year={2022}
}
@article{shojaee2023execution,
  title={Execution-based code generation using deep reinforcement learning},
  author={Shojaee, Parshin and Jain, Aneesh and Tipirneni, Sindhu and Reddy, Chandan K},
  journal={arXiv preprint arXiv:2301.13816},
  year={2023}
}
@article{liu2023rltf,
  title={Rltf: Reinforcement learning from unit test feedback},
  author={Liu, Jiate and Zhu, Yiqin and Xiao, Kaiwen and Fu, Qiang and Han, Xiao and Yang, Wei and Ye, Deheng},
  journal={arXiv preprint arXiv:2307.04349},
  year={2023}
}
@article{christopoulou2022pangu,
  title={Pangu-coder: Program synthesis with function-level language modeling},
  author={Christopoulou, Fenia and Lampouras, Gerasimos and Gritta, Milan and Zhang, Guchun and Guo, Yinpeng and Li, Zhongqi and Zhang, Qi and Xiao, Meng and Shen, Bo and Li, Lin and others},
  journal={arXiv preprint arXiv:2207.11280},
  year={2022}
}
@article{shen2023pangu,
  title={Pangu-coder2: Boosting large language models for code with ranking feedback},
  author={Shen, Bo and Zhang, Jiaxin and Chen, Taihong and Zan, Daoguang and Geng, Bing and Fu, An and Zeng, Muhan and Yu, Ailun and Ji, Jichuan and Zhao, Jingyang and others},
  journal={arXiv preprint arXiv:2307.14936},
  year={2023}
}
@article{le2023bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  year={2023}
}
@inproceedings{zheng2023codegeex,
  title={Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x},
  author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Shen, Lei and Wang, Zihan and Wang, Andi and Li, Yang and others},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={5673--5684},
  year={2023}
}
@article{zan2022cert,
  title={CERT: continual pre-training on sketches for library-oriented code generation},
  author={Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},
  journal={arXiv preprint arXiv:2206.06888},
  year={2022}
}
@article{wang2023aligning,
  title={Aligning large language models with human: A survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2307.12966},
  year={2023}
}
@article{yuan2023rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}
@article{laurenccon2022bigscience,
  title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31809--31826},
  year={2022}
}
@inproceedings{al2024traces,
  title={Traces of Memorisation in Large Language Models for Code},
  author={Al-Kaswan, Ali and Izadi, Maliheh and Van Deursen, Arie},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}
@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}
@article{zhao2023length,
  title={Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding},
  author={Zhao, Liang and Feng, Xiaocheng and Feng, Xiachong and Qin, Bin and Liu, Ting},
  journal={arXiv preprint arXiv:2312.17044},
  year={2023}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@article{kocetkov2022stack,
  title={The Stack: 3 TB of permissively licensed source code},
  author={Kocetkov, Denis and Li, Raymond and Jia, LI and Mou, Chenghao and Jernite, Yacine and Mitchell, Margaret and Ferrandis, Carlos Mu{\~n}oz and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}
@inproceedings{guo2023longcoder,
  title={Longcoder: A long-range pre-trained language model for code completion},
  author={Guo, Daya and Xu, Canwen and Duan, Nan and Yin, Jian and McAuley, Julian},
  booktitle={International Conference on Machine Learning},
  pages={12098--12107},
  year={2023},
  organization={PMLR}
}
@inproceedings{wang2021code,
  title={Code completion by modeling flattened abstract syntax trees as graphs},
  author={Wang, Yanlin and Li, Hui},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={16},
  pages={14015--14023},
  year={2021}
}
@inproceedings{liu2021retrieval,
  title={Retrieval-augmented generation for code summarization via hybrid GNN.(2021)},
  author={LIU, Shangqing and CHEN, Yu and XIE, Xiaofei and SIOW, Jingkai and LIU, Yang},
  booktitle={Proceedings of the Ninth International Conference on Learning Representations: ICLR},
  pages={4--8},
  year={2021}
}
@article{wu2024repoformer,
  title={Repoformer: Selective Retrieval for Repository-Level Code Completion},
  author={Wu, Di and Ahmad, Wasi Uddin and Zhang, Dejiao and Ramanathan, Murali Krishna and Ma, Xiaofei},
  journal={arXiv preprint arXiv:2403.10059},
  year={2024}
}
@inproceedings{lu2022reacc,
  title={ReACC: A Retrieval-Augmented Code Completion Framework},
  author={Lu, Shuai and Duan, Nan and Han, Hojae and Guo, Daya and Hwang, Seung-won and Svyatkovskiy, Alexey},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6227--6240},
  year={2022}
}
@misc{codeup,
  author = {Juyong Jiang and Sunghun Kim},
  title = {CodeUp: A Multilingual Code Generation Llama2 Model with Parameter-Efficient Instruction-Tuning},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/juyongjiang/CodeUp}},
}
@misc{Devin,
  author = {Cognition},
  title = {Introducing Devin, the first AI software engineer},
  year = {2024},
  publisher = {Cognition},
  journal = {Cognition Blog},
  howpublished = {\url{https://www.cognition.ai/introducing-devin}},
}
@misc{OpenDevin,
  author = {OpenDevin},
  title = {OpenDevin: Code Less, Make More},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/OpenDevin/OpenDevin}},
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@misc{OpenDevin_CodeAct,
  author = {Xingyao Wang, Bowen Li, and Graham Neubig},
  title = {Introducing OpenDevin CodeAct 1.0, a new State-of-the-art in Coding Agents},
  year = {2024},
  publisher = {OpenDevin},
  journal = {OpenDevin Blog},
  howpublished = {\url{https://www.cognition.ai/introducing-devin}},
}
@misc{CodeWhisperer,
  author = {Amazon},
  title = {What is CodeWhisperer?},
  year = {2022},
  publisher = {Amazon},
  journal = {Amazon Blog},
  howpublished = {\url{https://docs.aws.amazon.com/codewhisperer/latest/userguide/what-is-cwspr.html}},
}
@misc{Codeium,
  author = {Codeium},
  title = {Free, ultrafast Copilot alternative for Vim and Neovim},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Exafunction/codeium.vim}},
}
@misc{TabNine,
  author = {TabNine},
  title = {AI Code Completions},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/codota/TabNine}},
}
@misc{Replit,
  author = {Replit},
  title = {Idea to software, fast},
  year = {2016},
  publisher = {Replit},
  howpublished = {\url{https://replit.com}},
}
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@inproceedings{chen2022transferability,
  title={On the transferability of pre-trained language models for low-resource programming languages},
  author={Chen, Fuxiang and Fard, Fatemeh H and Lo, David and Bryksin, Timofey},
  booktitle={Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
  pages={401--412},
  year={2022}
}
@inproceedings{lai2023ds,
  title={DS-1000: A natural and reliable benchmark for data science code generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle={International Conference on Machine Learning},
  pages={18319--18345},
  year={2023},
  organization={PMLR}
}
@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}
@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}
@article{cassano2023knowledge,
  title={Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs},
  author={Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Anderson, Carolyn Jane and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun},
  journal={arXiv preprint arXiv:2308.09895},
  year={2023}
}
@inproceedings{zan2023large,
  title={Large Language Models Meet NL2Code: A Survey},
  author={Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Yongji, Wang and Lou, Jian-Guang},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7443--7464},
  year={2023}
}
@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}
@article{jelinek1977perplexity,
  title={Perplexityâ€”a measure of the difficulty of speech recognition tasks},
  author={Jelinek, Fred and Mercer, Robert L and Bahl, Lalit R and Baker, James K},
  journal={The Journal of the Acoustical Society of America},
  volume={62},
  number={S1},
  pages={S63--S63},
  year={1977},
  publisher={Acoustical Society of America}
}
@inproceedings{raemaekers2012measuring,
  title={Measuring software library stability through historical version analysis},
  author={Raemaekers, Steven and Van Deursen, Arie and Visser, Joost},
  booktitle={2012 28th IEEE international conference on software maintenance (ICSM)},
  pages={378--387},
  year={2012},
  organization={IEEE}
}
@inproceedings{peitek2021program,
  title={Program comprehension and code complexity metrics: An fmri study},
  author={Peitek, Norman and Apel, Sven and Parnin, Chris and Brechmann, Andr{\'e} and Siegmund, Janet},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages={524--536},
  year={2021},
  organization={IEEE}
}
@article{ardito2020tool,
  title={A tool-based perspective on software code maintainability metrics: a systematic literature review},
  author={Ardito, Luca and Coppola, Riccardo and Barbato, Luca and Verga, Diego},
  journal={Scientific Programming},
  volume={2020},
  pages={1--26},
  year={2020},
  publisher={Hindawi Limited}
}
@article{buse2009learning,
  title={Learning a metric for code readability},
  author={Buse, Raymond PL and Weimer, Westley R},
  journal={IEEE Transactions on software engineering},
  volume={36},
  number={4},
  pages={546--558},
  year={2009},
  publisher={IEEE}
}
@inproceedings{nappa2015attack,
  title={The attack of the clones: A study of the impact of shared code on vulnerability patching},
  author={Nappa, Antonio and Johnson, Richard and Bilge, Leyla and Caballero, Juan and Dumitras, Tudor},
  booktitle={2015 IEEE symposium on security and privacy},
  pages={692--708},
  year={2015},
  organization={IEEE}
}
@inproceedings{markovtsev2019style,
  title={STYLE-ANALYZER: fixing code style inconsistencies with interpretable unsupervised algorithms},
  author={Markovtsev, Vadim and Long, Waren and Mougard, Hugo and Slavnov, Konstantin and Bulychev, Egor},
  booktitle={2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)},
  pages={468--478},
  year={2019},
  organization={IEEE}
}
@inproceedings{olausson2023self,
  title={Is Self-Repair a Silver Bullet for Code Generation?},
  author={Olausson, Theo X and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng and Solar-Lezama, Armando},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{bairi2023codeplan,
  title={Codeplan: Repository-level coding using llms and planning},
  author={Bairi, Ramakrishna and Sonwane, Atharv and Kanade, Aditya and Iyer, Arun and Parthasarathy, Suresh and Rajamani, Sriram and Ashok, B and Shet, Shashank and others},
  journal={arXiv preprint arXiv:2309.12499},
  year={2023}
}
@inproceedings{shrivastava2023repository,
  title={Repository-level prompt generation for large language models of code},
  author={Shrivastava, Disha and Larochelle, Hugo and Tarlow, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={31693--31715},
  year={2023},
  organization={PMLR}
}
@article{shrivastava2023repofusion,
  title={RepoFusion: Training Code Models to Understand Your Repository},
  author={Shrivastava, Disha and Kocetkov, Denis and de Vries, Harm and Bahdanau, Dzmitry and Scholak, Torsten},
  journal={arXiv preprint arXiv:2306.10998},
  year={2023}
}
@article{chen2022codet,
  title={Codet: Code generation with generated tests},
  author={Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2207.10397},
  year={2022}
}
@article{wu2024repoformer,
  title={Repoformer: Selective Retrieval for Repository-Level Code Completion},
  author={Wu, Di and Ahmad, Wasi Uddin and Zhang, Dejiao and Ramanathan, Murali Krishna and Ma, Xiaofei},
  journal={arXiv preprint arXiv:2403.10059},
  year={2024}
}
@article{zan2024codes,
  title={CodeS: Natural Language to Code Repository via Multi-Layer Sketch},
  author={Zan, Daoguang and Yu, Ailun and Liu, Wei and Chen, Dong and Shen, Bo and Li, Wei and Yao, Yafen and Gong, Yongshun and Chen, Xiaolin and Guan, Bei and others},
  journal={arXiv preprint arXiv:2403.16443},
  year={2024}
}
@article{phan2024repohyper,
  title={RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion},
  author={Phan, Huy N and Phan, Hoang N and Nguyen, Tien N and Bui, Nghi DQ},
  journal={arXiv preprint arXiv:2403.06095},
  year={2024}
}
@article{kitchenham2009systematic,
  title={Systematic literature reviews in software engineering--a systematic literature review},
  author={Kitchenham, Barbara and Brereton, O Pearl and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
  journal={Information and software technology},
  volume={51},
  number={1},
  pages={7--15},
  year={2009},
  publisher={Elsevier}
}
@article{hoffa2016github,
  title={GitHub on BigQuery: Analyze all the open source code},
  author={Hoffa, Felipe},
  journal={URL: \url{https://cloud. google. com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-source-code}},
  year={2016}
}
@article{wang2022recode,
  title = {ReCode: Robustness Evaluation of Code Generation Models},
  author = {Wang, Shiqi and
   Zheng, Li and
   Qian, Haifeng and
   Yang, Chenghao and
   Wang, Zijian and
   Kumar, Varun and
   Shang, Mingyue and
   Tan, Samson and
   Ray, Baishakhi and
   Bhatia, Parminder and
   Nallapati, Ramesh and
   Ramanathan, Murali Krishna and
   Roth, Dan and
   Xiang, Bing},
  doi = {10.48550/arXiv.2212.10264},
  url = {https://arxiv.org/abs/2212.10264},
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL)},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}
@misc{babe2023studenteval,
      title={StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code}, 
      author={Hannah McLean Babe and Sydney Nguyen and Yangtian Zi and Arjun Guha and Molly Q Feldman and Carolyn Jane Anderson},
      year={2023},
      eprint={2306.04556},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{yu2024codereval,
  title={Codereval: A benchmark of pragmatic code generation with generative pre-trained models},
  author={Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao},
  booktitle={Proceedings of the 46th IEEE/ACM International Conference on Software Engineering},
  pages={1--12},
  year={2024}
}
@article{khan2023xcodeeval,
  title={xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval},
  author={Khan, Mohammad Abdullah Matin and Bari, M Saiful and Do, Xuan Long and Wang, Weishi and Parvez, Md Rizwan and Joty, Shafiq},
  journal={arXiv preprint arXiv:2303.03004},
  year={2023}
}
@article{wang2022execution,
  title={Execution-based evaluation for open-domain code generation},
  author={Wang, Zhiruo and Zhou, Shuyan and Fried, Daniel and Neubig, Graham},
  journal={arXiv preprint arXiv:2212.10481},
  year={2022}
}
@article{huang2022execution,
  title={Execution-based evaluation for data science code generation models},
  author={Huang, Junjie and Wang, Chenglong and Zhang, Jipeng and Yan, Cong and Cui, Haotian and Inala, Jeevana Priya and Clement, Colin and Duan, Nan and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2211.09374},
  year={2022}
}
@article{chandel2022training,
  title={Training and evaluating a jupyter notebook data science assistant},
  author={Chandel, Shubham and Clement, Colin B and Serrato, Guillermo and Sundaresan, Neel},
  journal={arXiv preprint arXiv:2201.12901},
  year={2022}
}
@article{husain2019codesearchnet,
  title={Codesearchnet challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}
@book{tunstall2022natural,
  title={Natural language processing with transformers},
  author={Tunstall, Lewis and Von Werra, Leandro and Wolf, Thomas},
  year={2022},
  publisher={" O'Reilly Media, Inc."}
}
@article{ding2022cocomic,
  title={Cocomic: Code completion by jointly modeling in-file and cross-file context},
  author={Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi Uddin and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and Xiang, Bing},
  journal={arXiv preprint arXiv:2212.10007},
  year={2022}
}
@article{bird2022taking,
  title={Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools},
  author={Bird, Christian and Ford, Denae and Zimmermann, Thomas and Forsgren, Nicole and Kalliamvakou, Eirini and Lowdermilk, Travis and Gazit, Idan},
  journal={Queue},
  volume={20},
  number={6},
  pages={35--57},
  year={2022},
  publisher={ACM New York, NY, USA}
}
@article{barke2023grounded,
  title={Grounded copilot: How programmers interact with code-generating models},
  author={Barke, Shraddha and James, Michael B and Polikarpova, Nadia},
  journal={Proceedings of the ACM on Programming Languages},
  volume={7},
  number={OOPSLA1},
  pages={85--111},
  year={2023},
  publisher={ACM New York, NY, USA}
}
@article{mozannar2022reading,
  title={Reading between the lines: Modeling user behavior and costs in AI-assisted programming},
  author={Mozannar, Hussein and Bansal, Gagan and Fourney, Adam and Horvitz, Eric},
  journal={arXiv preprint arXiv:2210.14306},
  year={2022}
}
@inproceedings{vaithilingam2022expectation,
  title={Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models},
  author={Vaithilingam, Priyan and Zhang, Tianyi and Glassman, Elena L},
  booktitle={Chi conference on human factors in computing systems extended abstracts},
  pages={1--7},
  year={2022}
}
@article{li2024evocodebench,
  title={EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories},
  author={Li, Jia and Li, Ge and Zhang, Xuanming and Dong, Yihong and Jin, Zhi},
  journal={arXiv preprint arXiv:2404.00599},
  year={2024}
}
@article{liu2023repobench,
  title={Repobench: Benchmarking repository-level code auto-completion systems},
  author={Liu, Tianyang and Xu, Canwen and McAuley, Julian},
  journal={arXiv preprint arXiv:2306.03091},
  year={2023}
}
@article{shinn2024reflexion,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{ni2023lever,
  title={Lever: Learning to verify language-to-code generation with execution},
  author={Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Veselin and Yih, Wen-tau and Wang, Sida and Lin, Xi Victoria},
  booktitle={International Conference on Machine Learning},
  pages={26106--26128},
  year={2023},
  organization={PMLR}
}
@article{su2024arks,
  title={ARKS: Active Retrieval in Knowledge Soup for Code Generation},
  author={Su, Hongjin and Jiang, Shuyang and Lai, Yuhang and Wu, Haoyuan and Shi, Boao and Liu, Che and Liu, Qian and Yu, Tao},
  journal={arXiv preprint arXiv:2402.12317},
  year={2024}
}
@article{rajkumar2022evaluating,
  title={Evaluating the text-to-sql capabilities of large language models},
  author={Rajkumar, Nitarshan and Li, Raymond and Bahdanau, Dzmitry},
  journal={arXiv preprint arXiv:2204.00498},
  year={2022}
}
@article{jiang2023selfevolve,
  title={Selfevolve: A code evolution framework via large language models},
  author={Jiang, Shuyang and Wang, Yuhao and Wang, Yu},
  journal={arXiv preprint arXiv:2306.02907},
  year={2023}
}
@article{chen2023teaching,
  title={Teaching large language models to self-debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}
@article{laurenccon2022bigscience,
  title={The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31809--31826},
  year={2022}
}
@article{kocetkov2022stack,
  title={The stack: 3 tb of permissively licensed source code},
  author={Kocetkov, Denis and Li, Raymond and Allal, Loubna Ben and Li, Jia and Mou, Chenghao and Ferrandis, Carlos Mu{\~n}oz and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and others},
  journal={arXiv preprint arXiv:2211.15533},
  year={2022}
}
@article{zan2022cert,
  title={CERT: continual pre-training on sketches for library-oriented code generation},
  author={Zan, Daoguang and Chen, Bei and Yang, Dejian and Lin, Zeqi and Kim, Minsu and Guan, Bei and Wang, Yongji and Chen, Weizhu and Lou, Jian-Guang},
  journal={arXiv preprint arXiv:2206.06888},
  year={2022}
}
@article{zheng2023codegeex,
  title={Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x},
  author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Wang, Zihan and Shen, Lei and Wang, Andi and Li, Yang and others},
  journal={arXiv preprint arXiv:2303.17568},
  year={2023}
}
@article{cassano2022scalable,
  title={A scalable and extensible approach to benchmarking nl2code for 18 programming languages},
  author={Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and others},
  journal={arXiv preprint arXiv:2208.08227},
  year={2022}
}
@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}
@article{chen2018tree,
  title={Tree-to-tree neural networks for program translation},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}
@misc{oa-leet10k,
  author = {Cognitive Computations},
  title = {{oa\_leet10k}},
  howpublished = {\url{https://huggingface.co/datasets/cognitivecomputations/oa_leet10k}},
  year = 2023,
}
@misc{evol_instruction,
  author = {Nick Roshdieh},
  title = {{Evol-Instruct-Code-80k}},
  howpublished = {\url{https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1}},
  year = 2023,
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@software{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  note         = {{If you use this software, please cite it using 
                   these metadata.}},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@article{lu2021codexglue,
  title={Codexglue: A machine learning benchmark dataset for code understanding and generation},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  journal={arXiv preprint arXiv:2102.04664},
  year={2021}
}
@inproceedings{svyatkovskiy2020intellicode,
  title={Intellicode compose: Code generation using transformer},
  author={Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
  booktitle={Proceedings of the 28th ACM joint meeting on European software engineering conference and symposium on the foundations of software engineering},
  pages={1433--1443},
  year={2020}
}
@article{gong2024ast,
  title={AST-T5: Structure-Aware Pretraining for Code Generation and Understanding},
  author={Gong, Linyuan and Elhoushi, Mostafa and Cheung, Alvin},
  journal={arXiv preprint arXiv:2401.03003},
  year={2024}
}
@inproceedings{zhang2022automated,
  title={Automated feedback generation for competition-level code},
  author={Zhang, Jialu and Li, De and Kolesar, John Charles and Shi, Hanyuan and Piskac, Ruzica},
  booktitle={Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
  pages={1--13},
  year={2022}
}
@article{singh2023codefusion,
  title={Codefusion: A pre-trained diffusion model for code generation},
  author={Singh, Mukul and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Verbruggen, Gust},
  journal={arXiv preprint arXiv:2310.17680},
  year={2023}
}
@article{chai2022ernie,
  title={ERNIE-Code: Beyond english-centric cross-lingual pretraining for programming languages},
  author={Chai, Yekun and Wang, Shuohuan and Pang, Chao and Sun, Yu and Tian, Hao and Wu, Hua},
  journal={arXiv preprint arXiv:2212.06742},
  year={2022}
}
@article{liu2024best,
  title={Best Practices and Lessons Learned on Synthetic Data for Language Models},
  author={Liu, Ruibo and Wei, Jerry and Liu, Fangyu and Si, Chenglei and Zhang, Yanzhe and Rao, Jinmeng and Zheng, Steven and Peng, Daiyi and Yang, Diyi and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2404.07503},
  year={2024}
}
@inproceedings{wang2023self,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={The 61st Annual Meeting Of The Association For Computational Linguistics},
  year={2023}
}
@article{yang2024robustness,
  title={Robustness, security, privacy, explainability, efficiency, and usability of large language models for code},
  author={Yang, Zhou and Sun, Zhensu and Yue, Terry Zhuo and Devanbu, Premkumar and Lo, David},
  journal={arXiv preprint arXiv:2403.07506},
  year={2024}
}
@article{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}
@article{wang2023knowledge,
  title={Knowledge editing for large language models: A survey},
  author={Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and others},
  journal={arXiv preprint arXiv:2310.16218},
  year={2023}
}
@article{cassano2023knowledge,
  title={Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs},
  author={Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Anderson, Carolyn Jane and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun},
  journal={arXiv preprint arXiv:2308.09895},
  year={2023}
}
@inproceedings{chen2022transferability,
  title={On the transferability of pre-trained language models for low-resource programming languages},
  author={Chen, Fuxiang and Fard, Fatemeh H and Lo, David and Bryksin, Timofey},
  booktitle={Proceedings of the 30th IEEE/ACM International Conference on Program Comprehension},
  pages={401--412},
  year={2022}
}
@inproceedings{thakur2023benchmarking,
  title={Benchmarking large language models for automated verilog rtl code generation},
  author={Thakur, Shailja and Ahmad, Baleegh and Fan, Zhenxing and Pearce, Hammond and Tan, Benjamin and Karri, Ramesh and Dolan-Gavitt, Brendan and Garg, Siddharth},
  booktitle={2023 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}
@article{ishida2024langprop,
  title={LangProp: A code optimization framework using Language Models applied to driving},
  author={Ishida, Shu and Corrado, Gianluca and Fedoseev, George and Yeo, Hudson and Russell, Lloyd and Shotton, Jamie and Henriques, Jo{\~a}o F and Hu, Anthony},
  journal={arXiv preprint arXiv:2401.10314},
  year={2024}
}
@article{shirafuji2023refactoring,
  title={Refactoring Programs Using Large Language Models with Few-Shot Examples},
  author={Shirafuji, Atsushi and Oda, Yusuke and Suzuki, Jun and Morishita, Makoto and Watanobe, Yutaka},
  journal={arXiv preprint arXiv:2311.11690},
  year={2023}
}
@article{zhong2024ldb,
  title={LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step},
  author={Zhong, Li and Wang, Zilong and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.16906},
  year={2024}
}
@article{kopf2024openassistant,
  title={Openassistant conversations-democratizing large language model alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi Rui and Stevens, Keith and Barhoum, Abdullah and Nguyen, Duc and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{wang2023aligning,
  title={Aligning large language models with human: A survey},
  author={Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2307.12966},
  year={2023}
}
@article{meng2022generating,
  title={Generating training data with language models: Towards zero-shot language understanding},
  author={Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={462--477},
  year={2022}
}
@article{zhong2024ldb,
  title={Ldb: A large language model debugger via verifying runtime execution step-by-step},
  author={Zhong, Li and Wang, Zilong and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.16906},
  year={2024}
}
@article{wettig2024qurating,
  title={QuRating: Selecting High-Quality Data for Training Language Models},
  author={Wettig, Alexander and Gupta, Aatmik and Malik, Saumya and Chen, Danqi},
  journal={arXiv preprint arXiv:2402.09739},
  year={2024}
}
@article{zhou2024lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{joshi2023repair,
  title={Repair is nearly generation: Multilingual program repair with llms},
  author={Joshi, Harshit and Sanchez, Jos{\'e} Cambronero and Gulwani, Sumit and Le, Vu and Verbruggen, Gust and Radi{\v{c}}ek, Ivan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={4},
  pages={5131--5140},
  year={2023}
}
@article{lyu2024automatic,
  title={Automatic Programming: Large Language Models and Beyond},
  author={Lyu, Michael R and Ray, Baishakhi and Roychoudhury, Abhik and Tan, Shin Hwei and Thongtanunam, Patanamon},
  journal={arXiv preprint arXiv:2405.02213},
  year={2024}
}
@inproceedings{fan2023automated,
  title={Automated repair of programs from large language models},
  author={Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={1469--1481},
  year={2023},
  organization={IEEE}
}
@article{xu2024aligning,
  title={Aligning LLMs for FL-free Program Repair},
  author={Xu, Junjielong and Fu, Ying and Tan, Shin Hwei and He, Pinjia},
  journal={arXiv preprint arXiv:2404.08877},
  year={2024}
}
@inproceedings{li2023classifying,
  title={Classifying Code Comments via Pre-trained Programming Language Model},
  author={Li, Ying and Wang, Haibo and Zhang, Huaien and Tan, Shin Hwei},
  booktitle={2023 IEEE/ACM 2nd International Workshop on Natural Language-Based Software Engineering (NLBSE)},
  pages={24--27},
  year={2023},
  organization={IEEE}
}
@inproceedings{xiong2017precise,
  title={Precise condition synthesis for program repair},
  author={Xiong, Yingfei and Wang, Jie and Yan, Runfa and Zhang, Jiachen and Han, Shi and Huang, Gang and Zhang, Lu},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)},
  pages={416--426},
  year={2017},
  organization={IEEE}
}
@inproceedings{ji2020question,
  title={Question selection for interactive program synthesis},
  author={Ji, Ruyi and Liang, Jingjing and Xiong, Yingfei and Zhang, Lu and Hu, Zhenjiang},
  booktitle={Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages={1143--1158},
  year={2020}
}
@article{lachaux2020unsupervised,
  title={Unsupervised translation of programming languages},
  author={Lachaux, Marie-Anne and Roziere, Baptiste and Chanussot, Lowik and Lample, Guillaume},
  journal={arXiv preprint arXiv:2006.03511},
  year={2020}
}
@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}
@article{liu2024your,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{du2024evaluating,
  title={Evaluating large language models in class-level code generation},
  author={Du, Xueying and Liu, Mingwei and Wang, Kaixin and Wang, Hanlin and Liu, Junwei and Chen, Yixuan and Feng, Jiayi and Sha, Chaofeng and Peng, Xin and Lou, Yiling},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}
@inproceedings{guo2022unixcoder,
  title={UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  author={Guo, Daya and Lu, Shuai and Duan, Nan and Wang, Yanlin and Zhou, Ming and Yin, Jian},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7212--7225},
  year={2022}
}
@article{guo2020graphcodebert,
  title={Graphcodebert: Pre-training code representations with data flow},
  author={Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Svyatkovskiy, Alexey and Fu, Shengyu and others},
  journal={arXiv preprint arXiv:2009.08366},
  year={2020}
}
@inproceedings{szafraniec2022code,
  title={Code translation with compiler representations},
  author={Szafraniec, Marc and Roziere, Baptiste and Leather, Hugh and Charton, Francois and Labatut, Patrick and Synnaeve, Gabriel},
  booktitle={Proceedings of the Eleventh International Conference on Learning Representations: ICLR},
  year={2022}
}
@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}
@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}
@article{zhong2024ldb,
  title={LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step},
  author={Zhong, Li and Wang, Zilong and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.16906},
  year={2024}
}
@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}
@article{fried2022incoder,
  title={Incoder: A generative model for code infilling and synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2204.05999},
  year={2022}
}
@article{palacio2023evaluating,
  title={Evaluating and explaining large language models for code using syntactic structures},
  author={Palacio, David N and Velasco, Alejandro and Rodriguez-Cardenas, Daniel and Moran, Kevin and Poshyvanyk, Denys},
  journal={arXiv preprint arXiv:2308.03873},
  year={2023}
}
@inproceedings{ahmad2020transformer,
  title={A Transformer-based Approach for Source Code Summarization},
  author={Ahmad, Wasi and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4998--5007},
  year={2020}
}
@article{xu2024first,
  title={A First Look at License Compliance Capability of LLMs in Code Generation},
  author={Xu, Weiwei and Gao, Kai and He, Hao and Zhou, Minghui},
  journal={arXiv preprint arXiv:2408.02487},
  year={2024}
}
@article{ji2023benchmarking,
  title={Benchmarking and explaining large language model-based code generation: A causality-centric approach},
  author={Ji, Zhenlan and Ma, Pingchuan and Li, Zongjie and Wang, Shuai},
  journal={arXiv preprint arXiv:2310.06680},
  year={2023}
}
@inproceedings{chen2023teaching,
  title={Teaching large language models to self-debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  booktitle={Proceedings of the Twelfth International Conference on Learning Representations: ICLR},
  year={2024}
}
@article{allal2023santacoder,
  title={SantaCoder: don't reach for the stars!},
  author={Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and others},
  journal={arXiv preprint arXiv:2301.03988},
  year={2023}
}
@article{xie2023data,
  title={Data selection for language models via importance resampling},
  author={Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy S},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34201--34227},
  year={2023}
}
@inproceedings{hamalainen2023evaluating,
  title={Evaluating large language models in generating synthetic hci research data: a case study},
  author={H{\"a}m{\"a}l{\"a}inen, Perttu and Tavast, Mikke and Kunnari, Anton},
  booktitle={Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2023}
}
@article{ren2020codebleu,
  title={Codebleu: a method for automatic evaluation of code synthesis},
  author={Ren, Shuo and Guo, Daya and Lu, Shuai and Zhou, Long and Liu, Shujie and Tang, Duyu and Sundaresan, Neel and Zhou, Ming and Blanco, Ambrosio and Ma, Shuai},
  journal={arXiv preprint arXiv:2009.10297},
  year={2020}
}
@inproceedings{zheng2023codegeex,
  title={Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x},
  author={Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Shen, Lei and Wang, Zihan and Wang, Andi and Li, Yang and others},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={5673--5684},
  year={2023}
}
@article{zheng2023survey,
  title={A survey of large language models for code: Evolution, benchmarking, and future trends},
  author={Zheng, Zibin and Ning, Kaiwen and Wang, Yanlin and Zhang, Jingwen and Zheng, Dewu and Ye, Mingxi and Chen, Jiachi},
  journal={arXiv preprint arXiv:2311.10372},
  year={2023}
}
@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{zhuo2024astraios,
  title={Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models},
  author={Zhuo, Terry Yue and Zebaze, Armel and Suppattarachai, Nitchakarn and von Werra, Leandro and de Vries, Harm and Liu, Qian and Muennighoff, Niklas},
  journal={arXiv preprint arXiv:2401.00788},
  year={2024}
}
@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{zhang2023adaptive,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}
@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
@article{zaken2021bitfit,
  title={Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2106.10199},
  year={2021}
}
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}
@article{wang2022machine,
  title={Machine/deep learning for software engineering: A systematic literature review},
  author={Wang, Simin and Huang, Liguo and Gao, Amiao and Ge, Jidong and Zhang, Tengfei and Feng, Haitao and Satyarth, Ishna and Li, Ming and Zhang, He and Ng, Vincent},
  journal={IEEE Transactions on Software Engineering},
  volume={49},
  number={3},
  pages={1188--1231},
  year={2022},
  publisher={IEEE}
}
@article{ramirez2018systematic,
  title={A systematic review of interaction in search-based software engineering},
  author={Ramirez, Aurora and Romero, Jose Raul and Simons, Christopher L},
  journal={IEEE Transactions on Software Engineering},
  volume={45},
  number={8},
  pages={760--781},
  year={2018},
  publisher={IEEE}
}
@article{liu2022deep,
  title={Deep learning for android malware defenses: a systematic literature review},
  author={Liu, Yue and Tantithamthavorn, Chakkrit and Li, Li and Liu, Yepang},
  journal={ACM Computing Surveys},
  volume={55},
  number={8},
  pages={1--36},
  year={2022},
  publisher={ACM New York, NY}
}
@article{li2017static,
  title={Static analysis of android apps: A systematic literature review},
  author={Li, Li and Bissyand{\'e}, Tegawend{\'e} F and Papadakis, Mike and Rasthofer, Siegfried and Bartel, Alexandre and Octeau, Damien and Klein, Jacques and Traon, Le},
  journal={Information and Software Technology},
  volume={88},
  pages={67--95},
  year={2017},
  publisher={Elsevier}
}
@misc{hou2024large,
      title={Large Language Models for Software Engineering: A Systematic Literature Review}, 
      author={Xinyi Hou and Yanjie Zhao and Yue Liu and Zhou Yang and Kailong Wang and Li Li and Xiapu Luo and David Lo and John Grundy and Haoyu Wang},
      year={2024},
      eprint={2308.10620},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@article{wang2024software,
  title={Software testing with large language models: Survey, landscape, and vision},
  author={Wang, Junjie and Huang, Yuchao and Chen, Chunyang and Liu, Zhe and Wang, Song and Wang, Qing},
  journal={IEEE Transactions on Software Engineering},
  year={2024},
  publisher={IEEE}
}
@inproceedings{li2022unleashing,
  title={Unleashing the power of compiler intermediate representation to enhance neural program embeddings},
  author={Li, Zongjie and Ma, Pingchuan and Wang, Huaijin and Wang, Shuai and Tang, Qiyi and Nie, Sen and Wu, Shi},
  booktitle={Proceedings of the 44th International Conference on Software Engineering},
  pages={2253--2265},
  year={2022}
}
@article{paul2024ircoder,
  title={IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators},
  author={Paul, Indraneil and Luo, Jun and Glava{\v{s}}, Goran and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2403.03894},
  year={2024}
}
@article{linstead2007mining,
  title={Mining internet-scale software repositories},
  author={Linstead, Erik and Rigor, Paul and Bajracharya, Sushil and Lopes, Cristina and Baldi, Pierre},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}
@inproceedings{reps2006intermediate,
  title={Intermediate-representation recovery from low-level code},
  author={Reps, Thomas and Balakrishnan, Gogul and Lim, Junghee},
  booktitle={Proceedings of the 2006 ACM SIGPLAN symposium on Partial evaluation and semantics-based program manipulation},
  pages={100--111},
  year={2006}
}
@article{ma2022code,
  title={Are Code Pre-trained Models Powerful to Learn Code Syntax and Semantics?},
  author={Ma, Wei and Zhao, Mengjie and Xie, Xiaofei and Hu, Qiang and Liu, Shangqing and Zhang, Jie and Wang, Wenhan and Liu, Yang},
  journal={arXiv preprint arXiv:2212.10017},
  year={2022}
}
@article{mou2014tbcnn,
  title={TBCNN: A tree-based convolutional neural network for programming language processing},
  author={Mou, Lili and Li, Ge and Jin, Zhi and Zhang, Lu and Wang, Tao},
  journal={arXiv preprint arXiv:1409.5718},
  year={2014}
}
@article{kou2023model,
  title={Is model attention aligned with human attention? an empirical study on large language models for code generation},
  author={Kou, Bonan and Chen, Shengmai and Wang, Zhijie and Ma, Lei and Zhang, Tianyi},
  journal={arXiv preprint arXiv:2306.01220},
  year={2023}
}
@article{nijkamp2023codegen2,
  title={Codegen2: Lessons for training llms on programming and natural languages},
  author={Nijkamp, Erik and Hayashi, Hiroaki and Xiong, Caiming and Savarese, Silvio and Zhou, Yingbo},
  journal={arXiv preprint arXiv:2305.02309},
  year={2023}
}
@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{clement2020pymt5,
  title={PyMT5: multi-mode translation of natural language and Python code with transformers},
  author={Clement, Colin B and Drain, Dawn and Timcheck, Jonathan and Svyatkovskiy, Alexey and Sundaresan, Neel},
  journal={arXiv preprint arXiv:2010.03150},
  year={2020}
}
@article{chandel2022training,
  title={Training and evaluating a jupyter notebook data science assistant},
  author={Chandel, Shubham and Clement, Colin B and Serrato, Guillermo and Sundaresan, Neel},
  journal={arXiv preprint arXiv:2201.12901},
  year={2022}
}
@inproceedings{jesse2021learning,
  title={Learning type annotation: Is big data enough?},
  author={Jesse, Kevin and Devanbu, Premkumar T and Ahmed, Toufique},
  booktitle={Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1483--1486},
  year={2021}
}
@inproceedings{parvez2021retrieval,
  title={Retrieval Augmented Code Generation and Summarization},
  author={Parvez, Md Rizwan and Ahmad, Wasi and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={2719--2734},
  year={2021}
}
@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}
@inproceedings{zhuo2024ice,
  title={ICE-Score: Instructing Large Language Models to Evaluate Code},
  author={Zhuo, Terry Yue},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  pages={2232--2242},
  year={2024}
}
@inproceedings{jimenez2023swe,
  title={SWE-bench: Can Language Models Resolve Real-world Github Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik R},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{ding2024crosscodeeval,
  title={Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion},
  author={Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi and Ding, Hantian and Tan, Ming and Jain, Nihal and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{hendrycks2021measuring,
  title={Measuring Coding Challenge Competence With APPS},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}
@article{huang2023agentcoder,
  title={AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation},
  author={Huang, Dong and Bu, Qingwen and Zhang, Jie M and Luck, Michael and Cui, Heming},
  journal={arXiv preprint arXiv:2312.13010},
  year={2023}
}
@inproceedings{olausson2023self,
  title={Is Self-Repair a Silver Bullet for Code Generation?},
  author={Olausson, Theo X and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng and Solar-Lezama, Armando},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{wang2024executable,
  title={Executable code actions elicit better llm agents},
  author={Wang, Xingyao and Chen, Yangyi and Yuan, Lifan and Zhang, Yizhe and Li, Yunzhu and Peng, Hao and Ji, Heng},
  journal={arXiv preprint arXiv:2402.01030},
  year={2024}
}
@article{zhang2024autocoderover,
  title={AutoCodeRover: Autonomous Program Improvement},
  author={Zhang, Yuntong and Ruan, Haifeng and Fan, Zhiyu and Roychoudhury, Abhik},
  journal={arXiv preprint arXiv:2404.05427},
  year={2024}
}
@inproceedings{holt2023l2mac,
  title={L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation},
  author={Holt, Samuel and Luyten, Max Ruiz and van der Schaar, Mihaela},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{ishibashi2024self,
  title={Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization},
  author={Ishibashi, Yoichi and Nishimura, Yoshimasa},
  journal={arXiv preprint arXiv:2404.02183},
  year={2024}
}
@article{zhang2024pydex,
  title={Pydex: Repairing bugs in introductory python assignments using llms},
  author={Zhang, Jialu and Cambronero, Jos{\'e} Pablo and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust},
  journal={Proceedings of the ACM on Programming Languages},
  volume={8},
  number={OOPSLA1},
  pages={1100--1124},
  year={2024},
  publisher={ACM New York, NY, USA}
}
@inproceedings{jiang2021cure,
  title={Cure: Code-aware neural machine translation for automatic program repair},
  author={Jiang, Nan and Lutellier, Thibaud and Tan, Lin},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages={1161--1173},
  year={2021},
  organization={IEEE}
}
@article{parasaram2024fact,
  title={The Fact Selection Problem in LLM-Based Program Repair},
  author={Parasaram, Nikhil and Yan, Huijie and Yang, Boyu and Flahy, Zineb and Qudsi, Abriele and Ziaber, Damian and Barr, Earl and Mechtaev, Sergey},
  journal={arXiv preprint arXiv:2404.05520},
  year={2024}
}
@article{shojaee2023execution,
  title={Execution-based Code Generation using Deep Reinforcement Learning},
  author={Shojaee, Parshin and Jain, Aneesh and Tipirneni, Sindhu and Reddy, Chandan K},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
@inproceedings{ni2023lever,
  title={Lever: Learning to verify language-to-code generation with execution},
  author={Ni, Ansong and Iyer, Srini and Radev, Dragomir and Stoyanov, Veselin and Yih, Wen-tau and Wang, Sida and Lin, Xi Victoria},
  booktitle={International Conference on Machine Learning},
  pages={26106--26128},
  year={2023},
  organization={PMLR}
}
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{chen2022codet,
  title={CodeT: Code Generation with Generated Tests},
  author={Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}
@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@inproceedings{wang2021codet5,
  title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={8696--8708},
  year={2021}
}
@inproceedings{guo2022unixcoder,
  title={UniXcoder: Unified Cross-Modal Pre-training for Code Representation},
  author={Guo, Daya and Lu, Shuai and Duan, Nan and Wang, Yanlin and Zhou, Ming and Yin, Jian},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={7212--7225},
  year={2022}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@inproceedings{wang2023codet5+,
  title={CodeT5+: Open Code Large Language Models for Code Understanding and Generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh and Bui, Nghi and Li, Junnan and Hoi, Steven},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={1069--1088},
  year={2023}
}
@article{cai2024survey,
  title={A survey on mixture of experts},
  author={Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2407.06204},
  year={2024}
}
@inproceedings{luo2023wizardcoder,
  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{liu2024deepseek,
  title={Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  author={Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}
@article{li2023towards,
  title={Towards enhancing in-context learning for code generation},
  author={Li, Jia and Zhao, Yunfei and Li, Yongmin and Li, Ge and Jin, Zhi},
  journal={arXiv preprint arXiv:2303.17780},
  year={2023}
}
@article{lin2022survey,
  title={A survey of transformers},
  author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  journal={AI open},
  volume={3},
  pages={111--132},
  year={2022},
  publisher={Elsevier}
}
@article{patel2023evaluating,
  title={Evaluating In-Context Learning of Libraries for Code Generation},
  author={Patel, Arkil and Reddy, Siva and Bahdanau, Dzmitry and Dasigi, Pradeep},
  journal={arXiv preprint arXiv:2311.09635},
  year={2023}
}
@inproceedings{athiwaratkun2022multi,
  title={Multi-lingual Evaluation of Code Generation Models},
  author={Athiwaratkun, Ben and Gouda, Sanjay Krishna and Wang, Zijian and Li, Xiaopeng and Tian, Yuchen and Tan, Ming and Ahmad, Wasi Uddin and Wang, Shiqi and Sun, Qing and Shang, Mingyue and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
@article{yoo2024hyperclova,
  title={HyperCLOVA X Technical Report},
  author={Yoo, Kang Min and Han, Jaegeun and In, Sookyo and Jeon, Heewon and Jeong, Jisu and Kang, Jaewook and Kim, Hyunwook and Kim, Kyung-Min and Kim, Munhyong and Kim, Sungju and others},
  journal={arXiv preprint arXiv:2404.01954},
  year={2024}
}
@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{kim2023solar,
  title={Solar 10.7 b: Scaling large language models with simple yet effective depth up-scaling},
  author={Kim, Dahyun and Park, Chanjun and Kim, Sanghoon and Lee, Wonsung and Song, Wonho and Kim, Yunsu and Kim, Hyeonwoo and Kim, Yungi and Lee, Hyeonju and Kim, Jihoo and others},
  journal={arXiv preprint arXiv:2312.15166},
  year={2023}
}
@article{yoo2024hyperclova,
  title={HyperCLOVA X Technical Report},
  author={Yoo, Kang Min and Han, Jaegeun and In, Sookyo and Jeon, Heewon and Jeong, Jisu and Kang, Jaewook and Kim, Hyunwook and Kim, Kyung-Min and Kim, Munhyong and Kim, Sungju and others},
  journal={arXiv preprint arXiv:2404.01954},
  year={2024}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}
@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}
@article{liu2023rltf,
  title={RLTF: Reinforcement Learning from Unit Test Feedback},
  author={Liu, Jiate and Zhu, Yiqin and Xiao, Kaiwen and FU, QIANG and Han, Xiao and Wei, Yang and Ye, Deheng},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{yuan2023rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}
@article{kim2024sdpo,
  title={sDPO: Don't Use Your Data All at Once},
  author={Kim, Dahyun and Kim, Yungi and Song, Wonho and Kim, Hyeonwoo and Kim, Yunsu and Kim, Sanghoon and Park, Chanjun},
  journal={arXiv preprint arXiv:2403.19270},
  year={2024}
}
@article{shojaee2023execution,
  title={Execution-based code generation using deep reinforcement learning},
  author={Shojaee, Parshin and Jain, Aneesh and Tipirneni, Sindhu and Reddy, Chandan K},
  journal={arXiv preprint arXiv:2301.13816},
  year={2023}
}
@article{liu2023rltf,
  title={Rltf: Reinforcement learning from unit test feedback},
  author={Liu, Jiate and Zhu, Yiqin and Xiao, Kaiwen and Fu, Qiang and Han, Xiao and Yang, Wei and Ye, Deheng},
  journal={arXiv preprint arXiv:2307.04349},
  year={2023}
}
@inproceedings{wang2022compilable,
  title={Compilable Neural Code Generation with Compiler Feedback},
  author={Wang, Xin and Wang, Yasheng and Wan, Yao and Mi, Fei and Li, Yitong and Zhou, Pingyi and Liu, Jin and Wu, Hao and Jiang, Xin and Liu, Qun},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2022},
  pages={9--19},
  year={2022}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{wang2024teaching,
  title={Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation},
  author={Wang, Chong and Zhang, Jian and Feng, Yebo and Li, Tianlin and Sun, Weisong and Liu, Yang and Peng, Xin},
  journal={arXiv preprint arXiv:2401.06391},
  year={2024}
}
@article{gunasekar2023textbooks,
  title={Textbooks are all you need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}
@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}
@article{li2023textbooks,
  title={Textbooks are all you need ii: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}
@inproceedings{zheng2023outline,
  title={Outline, then details: Syntactically guided coarse-to-fine code generation},
  author={Zheng, Wenqing and Sharan, SP and Jaiswal, Ajay Kumar and Wang, Kevin and Xi, Yihan and Xu, Dejia and Wang, Zhangyang},
  booktitle={International Conference on Machine Learning},
  pages={42403--42419},
  year={2023},
  organization={PMLR}
}
@article{li2023starcoder,
  title={Starcoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}
@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@article{lozhkov2024starcoder,
  title={StarCoder 2 and The Stack v2: The Next Generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}
@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}
@article{husain2019codesearchnet,
  title={Codesearchnet challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}
@article{guo2024deepseek,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Y and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}
@article{xie2024codeshell,
  title={CodeShell Technical Report},
  author={Xie, Rui and Zeng, Zhengran and Yu, Zhuohao and Gao, Chang and Zhang, Shikun and Ye, Wei},
  journal={arXiv preprint arXiv:2403.15747},
  year={2024}
}
@article{liu2023mftcoder,
  title={Mftcoder: Boosting code llms with multitask fine-tuning},
  author={Liu, Bingchang and Chen, Chaoyu and Liao, Cong and Gong, Zi and Wang, Huan and Lei, Zhichao and Liang, Ming and Chen, Dajun and Shen, Min and Zhou, Hailian and others},
  journal={arXiv preprint arXiv:2311.02303},
  year={2023}
}
@article{guo2024deepseek,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Y and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}
@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}
@inproceedings{wang2023self,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13484--13508},
  year={2023}
}
@misc{moritzlaurer,
  author = {Moritz Laurer},
  title = {{Synthetic data: save money, time and carbon with open source}},
  publisher = {Hugging Face},
  journal = {Hugging Face Blog},
  howpublished = "\url{https://huggingface.co/blog/synthetic-data-save-costs}",
  year = {2024}
}
@misc{claude3,
  author = {Anthropic},
  title = {{The Claude 3 Model Family: Opus, Sonnet, Haiku}},
  publisher = {Anthropic},
  howpublished = "\url{https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf}",
  year = {2024}
}
@misc{gpt-4o,
  author = {OpenAI},
  title = {{Hello GPT-4o}},
  publisher = {OpenAI},
  howpublished = "\url{https://openai.com/index/hello-gpt-4o/}",
  year = {2024}
}
@misc{evol-codealpaca-v1,
  author = {theblackcat102},
  title = {{The evolved code alpaca dataset}},
  publisher = {theblackcat102},
  howpublished = "\url{https://huggingface.co/datasets/ theblackcat102/evol-codealpaca-v1}",
  year = {2023}
}
@misc{gpt-4-turbo,
  author = {OpenAI},
  title = {{New models and developer products announced at DevDay}},
  publisher = {OpenAI},
  howpublished = "\url{https://openai.com/index/new-models-and-developer-products-announced-at-devday/}",
  year = {2024}
}
@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}
@misc{codestral,
  author = {MistralAI},
  title = {{Codestral}},
  publisher = {MistralAI},
  howpublished = "\url{https://mistral.ai/news/codestral/}",
  year = {2024}
}
@misc{phi-2,
  author = {Mojan Javaheripi, SÃ©bastien Bubeck},
  title = {{Phi-2: The surprising power of small language models}},
  publisher = {Microsoft},
  journal = {Microsoft Research Blog},
  howpublished = "\url{https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models}",
  year = {2023}
}
@misc{llama3,
  author = {Meta},
  title = {{Introducing Meta Llama 3: The most capable openly available LLM to date}},
  publisher = {Meta},
  journal = {Meta Blog},
  howpublished = "\url{https://ai.meta.com/blog/meta-llama-3/}",
  year = {2024}
}
@misc{codeparrot,
  author = {Hugging Face},
  title = {{Training CodeParrot from Scratch}},
  publisher = {Hugging Face},
  journal = {Hugging Face Blog},
  howpublished = "\url{https://github.com/huggingface/blog/blob/main/codeparrot.md}",
  year = {2023}
}
@inproceedings{yu2018spider,
  title={Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task},
  author={Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and others},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={3911--3921},
  year={2018}
}
@misc{replit-code,
  author = {Replit},
  title = {{replit-code-v1-3b}},
  publisher = {Replit},
  journal = {Hugging Face Models},
  howpublished = "\url{https://huggingface.co/replit/replit-code-v1-3b}",
  year = {2023}
}
@article{yu2023wavecoder,
  title={Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation},
  author={Yu, Zhaojian and Zhang, Xin and Shang, Ning and Huang, Yangyu and Xu, Can and Zhao, Yishujie and Hu, Wenxiang and Yin, Qiufeng},
  journal={arXiv preprint arXiv:2312.14187},
  year={2023}
}
@article{pinnaparaju2024stable,
  title={Stable Code Technical Report},
  author={Pinnaparaju, Nikhil and Adithyan, Reshinth and Phung, Duy and Tow, Jonathan and Baicoianu, James and Datta, Ashish and Zhuravinskyi, Maksym and Mahan, Dakota and Bellagente, Marco and Riquelme, Carlos and others},
  journal={arXiv preprint arXiv:2404.01226},
  year={2024}
}
@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@inproceedings{zhou2022least,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc V and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@inproceedings{huang2023towards,
  title={Towards Reasoning in Large Language Models: A Survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  booktitle={61st Annual Meeting of the Association for Computational Linguistics, ACL 2023},
  pages={1049--1065},
  year={2023},
  organization={Association for Computational Linguistics (ACL)}
}
@article{liang2023holistic,
  title={Holistic Evaluation of Language Models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
@inproceedings{jang2022towards,
  title={Towards Continual Knowledge Learning of Language Models},
  author={Jang, Joel and Ye, Seonghyeon and Yang, Sohee and Shin, Joongbo and Han, Janghoon and Kim, Gyeonghun and Choi, Jungkyu and Seo, Minjoon},
  booktitle={10th International Conference on Learning Representations, ICLR 2022},
  year={2022},
  organization={International Conference on Learning Representations}
}
@article{zhang2023siren,
  title={Siren's song in the AI ocean: a survey on hallucination in large language models},
  author={Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}
@article{ovadia2023fine,
  title={Fine-tuning or retrieval? comparing knowledge injection in llms},
  author={Ovadia, Oded and Brief, Menachem and Mishaeli, Moshik and Elisha, Oren},
  journal={arXiv preprint arXiv:2312.05934},
  year={2023}
}
@article{gupta2024rag,
  title={RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture},
  author={Gupta, Aman and Shirgaonkar, Anup and Balaguer, Angels de Luis and Silva, Bruno and Holstein, Daniel and Li, Dawei and Marsman, Jennifer and Nunes, Leonardo O and Rouzbahman, Mahsa and Sharp, Morris and others},
  journal={arXiv preprint arXiv:2401.08406},
  year={2024}
}
@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}
@article{huang2024position,
  title={Position Paper: Agent AI Towards a Holistic Intelligence},
  author={Huang, Qiuyuan and Wake, Naoki and Sarkar, Bidipta and Durante, Zane and Gong, Ran and Taori, Rohan and Noda, Yusuke and Terzopoulos, Demetri and Kuno, Noboru and Famoti, Ade and others},
  journal={arXiv preprint arXiv:2403.00833},
  year={2024}
}
@article{wu2023autogen,
  title={Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang, Li and Zhang, Xiaoyun and Wang, Chi},
  journal={arXiv preprint arXiv:2308.08155},
  year={2023}
}
@article{swe-agent,
  title={SWE-AGENT: AGENT-COMPUTER INTERFACES ENABLE AUTOMATED SOFTWARE ENGINEERING},
  author={John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, Ofir Press},
  url="https://swe-agent.com/",
  year={2024}
}
@article{codegemma_2024,
    title={CodeGemma: Open Code Models Based on Gemma},
    url={https://goo.gle/codegemma},
    author={ {CodeGemma Team} and Hartman, Ale Jakse and Hu, Andrea and Choquette-Choo, Christopher A. and Zhao, Heri and Fine, Jane and Hui,
    Jeffrey and Shen, Jingyue and Kelley, Joe and Howland, Joshua and Bansal, Kshitij and Vilnis, Luke and Wirth, Mateo and Nguyen, Nam, and Michel, Paul and Choy, Peter and Joshi, Pratik and Kumar, Ravin and Hashmi, Sarmad and Agrawal, Shubham and Zuo, Siqi and Warkentin, Tris and Gong, Zhitao et al.},
    year={2024}
}
@article{zhao2023length,
  title={Length Extrapolation of Transformers: A Survey from the Perspective of Position Encoding},
  author={Zhao, Liang and Feng, Xiaocheng and Feng, Xiachong and Qin, Bin and Liu, Ting},
  journal={arXiv preprint arXiv:2312.17044},
  year={2023}
}
@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}
@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}
@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}
@inproceedings{chen2023duetcs,
  title={DUETCS: Code Style Transfer through Generation and Retrieval},
  author={Chen, Binger and Abedjan, Ziawasch},
  booktitle={2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)},
  pages={2362--2373},
  year={2023},
  organization={IEEE}
}
@article{yang2021quality,
  title={Quality assessment in systematic literature reviews: A software engineering perspective},
  author={Yang, Lanxin and Zhang, He and Shen, Haifeng and Huang, Xin and Zhou, Xin and Rong, Guoping and Shao, Dong},
  journal={Information and Software Technology},
  volume={130},
  pages={106397},
  year={2021},
  publisher={Elsevier}
}
@article{ren2020codebleu,
  title={Codebleu: a method for automatic evaluation of code synthesis},
  author={Ren, Shuo and Guo, Daya and Lu, Shuai and Zhou, Long and Liu, Shujie and Tang, Duyu and Sundaresan, Neel and Zhou, Ming and Blanco, Ambrosio and Ma, Shuai},
  journal={arXiv preprint arXiv:2009.10297},
  year={2020}
}
@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}
@article{chen1998evaluation,
  title={Evaluation metrics for language models},
  author={Chen, Stanley F and Beeferman, Douglas and Rosenfeld, Roni},
  year={1998},
  publisher={Carnegie Mellon University}
}
@inproceedings{xu2022systematic,
  title={A systematic evaluation of large language models of code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
  booktitle={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  pages={1--10},
  year={2022}
}
@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}
@article{nijkamp2022codegen,
  title={Codegen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}
@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}
@article{xi2023rise,
  title={The rise and potential of large language model based agents: A survey},
  author={Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
  journal={arXiv preprint arXiv:2309.07864},
  year={2023}
}
@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={1--26},
  year={2024},
  publisher={Springer}
}
@article{hong2023metagpt,
  title={Metagpt: Meta programming for multi-agent collaborative framework},
  author={Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and others},
  journal={arXiv preprint arXiv:2308.00352},
  year={2023}
}
@inproceedings{zhou2022docprompting,
  title={DocPrompting: Generating Code by Retrieving the Docs},
  author={Zhou, Shuyan and Alon, Uri and Xu, Frank F and Jiang, Zhengbao and Neubig, Graham},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@article{weng2023agent,
  title   = "LLM-powered Autonomous Agents",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2023",
  month   = "Jun",
  url     = "https://lilianweng.github.io/posts/2023-06-23-agent/"
}
@inproceedings{zhang2023repocoder,
  title={RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation},
  author={Zhang, Fengji and Chen, Bei and Zhang, Yue and Keung, Jacky and Liu, Jin and Zan, Daoguang and Mao, Yi and Lou, Jian-Guang and Chen, Weizhu},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={2471--2484},
  year={2023}
}
@inproceedings{lu2021codexglue,
  title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  year={2021}
}
@inproceedings{lu2022reacc,
  title={ReACC: A Retrieval-Augmented Code Completion Framework},
  author={Lu, Shuai and Duan, Nan and Han, Hojae and Guo, Daya and Hwang, Seung-won and Svyatkovskiy, Alexey},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6227--6240},
  year={2022}
}
@inproceedings{liu2020retrieval,
  title={Retrieval-Augmented Generation for Code Summarization via Hybrid GNN},
  author={Liu, Shangqing and Chen, Yu and Xie, Xiaofei and Siow, Jing Kai and Liu, Yang},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{parvez2021retrieval,
  title={Retrieval Augmented Code Generation and Summarization},
  author={Parvez, Md Rizwan and Ahmad, Wasi and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={2719--2734},
  year={2021}
}
@inproceedings{chen2024benchmarking,
  title={Benchmarking large language models in retrieval-augmented generation},
  author={Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={17754--17762},
  year={2024}
}
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@misc{codealpaca,
  author = {Sahil Chaudhary},
  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sahil280114/codealpaca}},
}
@misc{autogpt,
  title = {AutoGPT is the vision of accessible AI for everyone, to use and to build on},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Significant-Gravitas/AutoGPT}},
}
@misc{agentgpt,
  title = {AgentGPT: Assemble, configure, and deploy autonomous AI Agents in your browser},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/reworkd/AgentGPT}},
}
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
@article{ni2023l2ceval,
  title={L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models},
  author={Ni, Ansong and Yin, Pengcheng and Zhao, Yilun and Riddell, Martin and Feng, Troy and Shen, Rui and Yin, Stephen and Liu, Ye and Yavuz, Semih and Xiong, Caiming and others},
  journal={arXiv preprint arXiv:2309.17446},
  year={2023}
}
@inproceedings{jha2010oracle,
  title={Oracle-guided component-based program synthesis},
  author={Jha, Susmit and Gulwani, Sumit and Seshia, Sanjit A and Tiwari, Ashish},
  booktitle={Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 1},
  pages={215--224},
  year={2010}
}
@inproceedings{gulwani2010dimensions,
  title={Dimensions in program synthesis},
  author={Gulwani, Sumit},
  booktitle={Proceedings of the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming},
  pages={13--24},
  year={2010}
}
@inproceedings{de2008z3,
  title={Z3: An efficient SMT solver},
  author={De Moura, Leonardo and Bj{\o}rner, Nikolaj},
  booktitle={International conference on Tools and Algorithms for the Construction and Analysis of Systems},
  pages={337--340},
  year={2008},
  organization={Springer}
}
@inproceedings{allamanis2014mining,
  title={Mining idioms from source code},
  author={Allamanis, Miltiadis and Sutton, Charles},
  booktitle={Proceedings of the 22nd acm sigsoft international symposium on foundations of software engineering},
  pages={472--483},
  year={2014}
}
@article{cohn2010inducing,
  title={Inducing tree-substitution grammars},
  author={Cohn, Trevor and Blunsom, Phil and Goldwater, Sharon},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={3053--3096},
  year={2010},
  publisher={JMLR. org}
}
@inproceedings{joshi2003formalism,
  title={A formalism for dependency grammar based on tree adjoining grammar},
  author={Joshi, Aravind and Rambow, Owen},
  booktitle={Proceedings of the Conference on Meaning-text Theory},
  pages={207--216},
  year={2003},
  organization={MTT Paris, France}
}
@article{wu2023autogen,
  title={Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Zhang, Shaokun and Zhu, Erkang and Li, Beibin and Jiang, Li and Zhang, Xiaoyun and Wang, Chi},
  journal={arXiv preprint arXiv:2308.08155},
  year={2023}
}
@misc{babyagi,
  title = {BabyAGI},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/yoheinakajima/babyagi}},
}
@article{zheng2024opencodeinterpreter,
  title={OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement},
  author={Zheng, Tianyu and Zhang, Ge and Shen, Tianhao and Liu, Xueling and Lin, Bill Yuchen and Fu, Jie and Chen, Wenhu and Yue, Xiang},
  journal={arXiv preprint arXiv:2402.14658},
  year={2024}
}
@article{ridnik2024code,
  title={Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering},
  author={Ridnik, Tal and Kredo, Dedy and Friedman, Itamar},
  journal={arXiv preprint arXiv:2401.08500},
  year={2024}
}
@article{dou2024stepcoder,
  title={StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback},
  author={Dou, Shihan and Liu, Yan and Jia, Haoxiang and Xiong, Limao and Zhou, Enyu and Shan, Junjie and Huang, Caishuang and Shen, Wei and Fan, Xiaoran and Xi, Zhiheng and others},
  journal={arXiv preprint arXiv:2402.01391},
  year={2024}
}
@article{bi2024iterative,
  title={Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback},
  author={Bi, Zhangqian and Wan, Yao and Wang, Zheng and Zhang, Hongyu and Guan, Batu and Lu, Fangxin and Zhang, Zili and Sui, Yulei and Shi, Xuanhua and Jin, Hai},
  journal={arXiv preprint arXiv:2403.16792},
  year={2024}
}
@misc{codeqwen,
  author = {Qwen Team},
  title = {Code with CodeQwen1.5},
  year = {2024},
  publisher = {Qwen},
  journal = {Qwen Blog},
  howpublished = {\url{https://qwenlm.github.io/blog/codeqwen1.5}},
}
@inproceedings{gao2023pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}
@misc{starcoder2instruct,
  author = {Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang},
  title = {StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/bigcode-project/starcoder2-self-align}},
}
@article{allal2023santacoder,
  title={SantaCoder: don't reach for the stars!},
  author={Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex and Dey, Manan and others},
  journal={arXiv preprint arXiv:2301.03988},
  year={2023}
}
@misc{gpt-3.5-turbo,
  author = {OpenAI},
  title = {Chatgpt: Optimizing language models for dialogue},
  year = {2022},
  publisher = {OpenAI},
  journal = {OpenAI Blog},
  howpublished = {\url{https://openai.com/blog/chatgpt}}
}
@inproceedings{van2023synthetic,
  title={Synthetic data, real errors: how (not) to publish and use synthetic data},
  author={Van Breugel, Boris and Qian, Zhaozhi and Van Der Schaar, Mihaela},
  booktitle={International Conference on Machine Learning},
  pages={34793--34808},
  year={2023},
  organization={PMLR}
}
@inproceedings{wood2021fake,
  title={Fake it till you make it: face analysis in the wild using synthetic data alone},
  author={Wood, Erroll and Baltru{\v{s}}aitis, Tadas and Hewitt, Charlie and Dziadzio, Sebastian and Cashman, Thomas J and Shotton, Jamie},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={3681--3691},
  year={2021}
}
@article{gupta2021transitioning,
  title={Transitioning from Real to Synthetic data: Quantifying the bias in model},
  author={Gupta, Aman and Bhatt, Deepak and Pandey, Anubha},
  journal={arXiv preprint arXiv:2105.04144},
  year={2021}
}
@article{barbierato2022methodology,
  title={A methodology for controlling bias and fairness in synthetic data generation},
  author={Barbierato, Enrico and Vedova, Marco L Della and Tessera, Daniele and Toti, Daniele and Vanoli, Nicola},
  journal={Applied Sciences},
  volume={12},
  number={9},
  pages={4619},
  year={2022},
  publisher={MDPI}
}
@article{babbar2019data,
  title={Data scarcity, robustness and extreme multi-label classification},
  author={Babbar, Rohit and Sch{\"o}lkopf, Bernhard},
  journal={Machine Learning},
  volume={108},
  number={8},
  pages={1329--1351},
  year={2019},
  publisher={Springer}
}
@misc{alphacode2,
  author = {AlphaCode Team, Google DeepMind},
  title = {{AlphaCode 2 Technical Report}},
  howpublished = {\url{https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf}},
  year = 2023,
  month = Dec
}
@article{wei2023magicoder,
  title={Magicoder: Source code is all you need},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  journal={arXiv preprint arXiv:2312.02120},
  year={2023}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{kocetkov2022stack,
  title={The Stack: 3 TB of permissively licensed source code},
  author={Kocetkov, Denis and Li, Raymond and Jia, LI and Mou, Chenghao and Jernite, Yacine and Mitchell, Margaret and Ferrandis, Carlos Mu{\~n}oz and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}
@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}
@article{cai2024shortcut,
  title={Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts},
  author={Cai, Weilin and Jiang, Juyong and Qin, Le and Cui, Junwei and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2404.05019},
  year={2024}
}
@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{zhu2024deepseek,
  title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
  author={Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y and Li, Yukun and Gao, Huazuo and Ma, Shirong and others},
  journal={arXiv preprint arXiv:2406.11931},
  year={2024}
}
@article{zhu2024deepseek,
  title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
  author={Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y and Li, Yukun and Gao, Huazuo and Ma, Shirong and others},
  journal={arXiv preprint arXiv:2406.11931},
  year={2024}
}
@article{hui2024qwen2,
  title={Qwen2. 5-coder technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}
@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}
@article{hui2024qwen2,
  title={Qwen2. 5-coder technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}
@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}
@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}
@article{zhou2023solving,
  title={Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification},
  author={Zhou, Aojun and Wang, Ke and Lu, Zimu and Shi, Weikang and Luo, Sichun and Qin, Zipeng and Lu, Shaoqing and Jia, Anya and Song, Linqi and Zhan, Mingjie and others},
  journal={arXiv preprint arXiv:2308.07921},
  year={2023}
}
@article{chen2022program,
  title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
  author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
  journal={arXiv preprint arXiv:2211.12588},
  year={2022}
}
@inproceedings{gao2023pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}
@inproceedings{fan2023large,
  title={Large language models for software engineering: Survey and open problems},
  author={Fan, Angela and Gokkaya, Beliz and Harman, Mark and Lyubarskiy, Mitya and Sengupta, Shubho and Yoo, Shin and Zhang, Jie M},
  booktitle={2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE)},
  pages={31--53},
  year={2023},
  organization={IEEE}
}
@article{yuan2023gpt,
  title={Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher},
  author={Yuan, Youliang and Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and He, Pinjia and Shi, Shuming and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2308.06463},
  year={2023}
}
@article{ahmad2021unified,
  title={Unified pre-training for program understanding and generation},
  author={Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2103.06333},
  year={2021}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}
@inproceedings{zhuo2024ice,
  title={ICE-Score: Instructing Large Language Models to Evaluate Code},
  author={Zhuo, Terry Yue},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2024},
  pages={2232--2242},
  year={2024}
}
@inproceedings{amini2019mathqa,
  title={MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms},
  author={Amini, Aida and Gabriel, Saadia and Lin, Shanchuan and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={2357--2367},
  year={2019}
}
@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}
@article{naman2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Naman Jain, King Han and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}
@article{gu2024cruxeval,
  title={Cruxeval: A benchmark for code reasoning, understanding and execution},
  author={Gu, Alex and Rozi{\`e}re, Baptiste and Leather, Hugh and Solar-Lezama, Armando and Synnaeve, Gabriel and Wang, Sida I},
  journal={arXiv preprint arXiv:2401.03065},
  year={2024}
}
@article{zhuo2024bigcodebench,
  title={Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions},
  author={Zhuo, Terry Yue and Vu, Minh Chien and Chim, Jenny and Hu, Han and Yu, Wenhao and Widyasari, Ratnadira and Yusuf, Imam Nur Bani and Zhan, Haolan and He, Junda and Paul, Indraneil and others},
  journal={arXiv preprint arXiv:2406.15877},
  year={2024}
}
@article{zhang2024naturalcodebench,
  title={NaturalCodeBench: Examining Coding Performance Mismatch on HumanEval and Natural User Prompts},
  author={Zhang, Shudan and Zhao, Hanlin and Liu, Xiao and Zheng, Qinkai and Qi, Zehan and Gu, Xiaotao and Zhang, Xiaohan and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2405.04520},
  year={2024}
}