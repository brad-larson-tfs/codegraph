Empowering AI to Generate Better AI Code: Guided
Generation of Deep Learning Projects with LLMs

Chen Xie, Mingsheng Jiao, Xiaodong Gu, Beijun Shen†

School of Computer Science, Shanghai Jiao Tong University, Shanghai, China
{dindin xc, jiaomingsheng, xiaodong.gu, bjshen}@sjtu.edu.cn

5
2
0
2

r
p
A
1
2

]
E
S
.
s
c
[

1
v
0
8
0
5
1
.
4
0
5
2
:
v
i
X
r
a

Abstract—While large language models (LLMs) have been
widely applied to code generation, they struggle with generating
entire deep learning projects, which are characterized by complex
structures, longer functions, and stronger reliance on domain
knowledge than general-purpose code. An open-domain LLM
often lacks coherent contextual guidance and domain expertise
for specific projects, making it challenging to produce complete
code that fully meets user requirements. In this paper, we propose
a novel planning-guided code generation method, DLCodeGen,
tailored for generating deep learning projects. DLCodeGen
predicts a structured solution plan, offering global guidance
for LLMs to generate the project. The generated plan is then
leveraged to retrieve semantically analogous code samples and
subsequently abstract a code template. To effectively integrate
these multiple retrieval-augmented techniques, a comparative
learning mechanism is designed to generate the final code. We
validate the effectiveness of our approach on a dataset we build
for deep learning code generation. Experimental results demon-
strate that DLCodeGen outperforms other baselines, achieving
improvements of 9.7% in CodeBLEU and 3.6% in human
evaluation metrics.

Index Terms—Deep learning code generation, solution plan-

ning, retrieval-augmented generation, comparative learning.

I. INTRODUCTION

Deep learning is a cornerstone methodology in artificial
intelligence, fundamentally centered on emulating the archi-
tecture and functionality of human neural networks to au-
tonomously learn and extract intricate patterns from data [1].
In recent years, the advent of the Transformer architecture [2]
has catalyzed groundbreaking advancements, enabling deep
learning to achieve unprecedented success across a diverse ar-
ray of domains, such as computer vision [3], natural language
processing [4], and software engineering [5]. However, the
development and optimization of deep learning projects remain
inherently intricate, demanding substantial domain-specific
expertise. Therefore, automated deep learning code generation
emerges as a promising paradigm to mitigate the challenges
faced by developers, reducing barriers to the construction and
deployment of deep learning models.

In recent years, pre-trained models have demonstrated ex-
ceptional performance in code generation tasks [6]–[11]. With
the advent of large language models (LLMs), their robust gen-
erative capabilities and adaptability have established them as

† Beijun Shen is the corresponding author.

the dominant paradigm in this field. However, unlike general-
purpose code, deep learning projects are characterized by their
complex structures, longer functions, and a strong reliance
on domain knowledge. Deep learning code typically encom-
passes more sophisticated logic, incorporating sequential steps
such as data preprocessing, model construction, compilation,
training, and model evaluation. These steps are highly interde-
pendent, forming an elaborate code chain that often exceeds
300 lines. Current LLMs exhibit a significant performance
degradation when generating code segments longer than 50
lines [12], and maintaining contextual coherence across such
extensive chains presents a considerable challenge. This poses
a significant obstacle for LLMs, which lack coherent contex-
tual guidance and domain expertise during the code generation
process.

Recent studies [13] have demonstrated that the chain-of-
thought (CoT) technique, which integrates intermediate rea-
soning steps into the prompt, can effectively guide LLMs in
accurately comprehending task requirements. Moreover, the
incorporation of programmatic structural information into the
reasoning chain has been shown to significantly improve the
performance of code generation tasks [14]–[16]. Additionally,
retrieval-augmented generation (RAG) [17] has emerged as a
highly effective optimization strategy. This approach leverages
the retrieval of relevant code snippets and supplementary
domain-specific knowledge to construct enriched prompts,
thereby addressing the inherent lack of domain expertise in
LLMs and reducing the occurrence of hallucinations that may
arise from this limitation.

Inspired by these techniques, we propose DLCodeGen, a
novel planning-guided method for deep learning code gen-
eration. Instead of presenting the reasoning process during
code generation, DLCodeGen prioritizes the formulation of
a meticulously structured solution plan prior to generation.
This plan acts as a global blueprint, effectively bridging the
gap between user requirements with the final code output
while ensuring coherence and alignment with domain-specific
constraints.

We train a GPT-2 language model as a solution plan
predictor using a parallel corpus specifically constructed for
this purpose. This model generates tailored solution plans
based on user requirements, offering comprehensive global
guidance for code generation. To further improve the quality of
the generated code, we implement two RAG strategies, namely

Code RAG and Template RAG, to produce intermediate re-
sults. These results are then integrated through a comparative
learning mechanism, which leverages the consistency ensured
by the solution plan to effectively combine the strengths of
both strategies, ultimately yielding optimized, contextually
coherent, and domain-aligned deep learning code.

To evaluate the effectiveness of DLCodeGen, we introduce
DLCodeEval, a benchmark specifically designed for deep
learning code generation, curated from open-source projects.
Experimental results show that DLCodeGen outperforms ex-
isting baselines, achieving substantial improvements of 9.7%
and 3.6% in CodeBLEU and human evaluation metrics, re-
spectively. Remarkably, the solution predictor exhibits superior
performance compared to LLMs with larger parameter scales.
Ablation studies further emphasize the pivotal role of the
proposed comparative learning mechanism and the dual RAG
strategies in driving the model’s success.

The contributions of our work are summarized as follows:
• We propose DLCodeGen, a novel planning-guided
method for deep learning code generation. DLCodeGen
employs a solution plan predictor to generate a structured
plan prior to code synthesis, offering global guidance that
ensures coherence and alignment with domain-specific
requirements throughout the generation process.

• We design a comparative learning mechanism that effec-
tively integrates the strengths of Code RAG and Template
RAG strategies. This mechanism enhances the quality
of the generated deep learning code by simultaneously
optimizing fine-grained implementation details and high-
level architectural logic.

• We construct DLCodeEval, a benchmark dataset for deep
learning code generation tasks, and compare DLCode-
Gen with state-of-the-art code generation approaches
using this dataset. Experimental results demonstrate that
DLCodeGen achieves significant performance improve-
ments, surpassing baselines by a considerable margin.

II. RELATED WORK

A. Generating Deep Learning Code

Research on the generation of deep learning code remains
relatively underexplored and is widely recognized as more
intricate than the generation of general-purpose code. Two
closely related research domains align with this objective:
low-code deep learning programming and automated machine
learning (AutoML).

Low-code programming aims to streamline the construc-
tion of artificial intelligence workflows, often through visual
programming interfaces [18]. However, proficiency in these
tools still necessitates substantial time and expertise, which
conflicts with the fundamental principles of low-code develop-
ment [19]. To address these limitations, Phothon Wizard [20]
integrates graphical user interfaces with dynamic code genera-
tion technologies, unifying multiple machine learning libraries
and providing modular explanations. LowCoder [21] intro-
duces an innovative paradigm that combines visual program-
ming with natural language programming, enabling automatic

API prediction and recommendation through natural language
descriptions. Text-to-ML [22] investigates the generation of
deep learning code from task descriptions, however its evalu-
ation is constrained by limited task diversity.

While these advancements have contributed to simplifying
deep learning programming, they still impose certain demands
on user expertise. Our research seeks to further lower the
development barrier by enabling the fully automated construc-
tion of deep learning projects while accommodating a broad
spectrum of complex tasks with flexibility.

AutoML [23] also strives to automate the deep learning
workflow. AutoML-GPT [24] utilizes LLMs to simulate model
evaluation, circumventing the need for actual execution. ML-
Copilot [25] enhances the generation of construction rec-
ommendations by extracting insights from knowledge bases,
while HuggingGPT [26] employs LLMs to orchestrate existing
deep learning models for task completion. However, these
studies primarily focus on identifying the most suitable model,
whereas our objective is to automatically generate comprehen-
sive deep learning code capable of executing specified tasks
with minimal user intervention.

B. Chain-of-Thought Prompting

With the rapid evolution of LLMs, Chain-of-thought
(CoT) [13] has emerged as a pivotal technique for enhanc-
ing code generation by incorporating logical reasoning steps
into prompts. S-CoT [14] further refines this approach by
structuring intermediate reasoning steps using programmatic
constructs such as sequences, branches, and loops, thereby
aligning more closely with the cognitive processes of develop-
ers. Auto-CoT [15] addresses the challenge of selecting high-
quality examples for few-shot and manual CoT by clustering
queries, identifying representative examples from each cluster,
and generating corresponding reasoning chains through zero-
shot prompting. Plan-and-Solve (PS) [27] mitigates common
issues in traditional reasoning chains, such as omitted steps
and semantic misinterpretations, by first decomposing prob-
lems into smaller, manageable subproblems and then solving
them in a sequential manner.

Other

research efforts harness the intrinsic knowledge
of LLMs to achieve self-improvement in code generation.
LDB [28] utilizes LLM-generated evaluations of code blocks
as iterative feedback to progressively optimize the output. Self-
Planning [16] employs few-shot prompting to derive solution
steps from problem intents and incrementally generates code
based on these plans. Similarly, SelfEvolve [29] implements
a two-stage framework, wherein the LLM first synthesizes
to the problem before
domain-specific knowledge relevant
generating the corresponding code.

In contrast to these studies that rely solely on the intrinsic
knowledge of LLMs to construct reasoning processes, DL-
CodeGen leverages a dedicated predictor trained specifically
to generate deep learning solution plans. By incorporating
domain-specific knowledge through fine-tuning, our approach
significantly enhances the accuracy and contextual relevance

Fig. 1: Overview of DLCodeGen

of the planning process, ensuring a more robust alignment with
the complexities of deep learning tasks.

C. Retrieval-Augmented Generation

Retrieval-augmented generation (RAG) [17] improves out-
put quality by retrieving relevant information from external
sources and integrating it with the original query. This ap-
proach is particularly effective for knowledge-intensive tasks
like code generation, as it mitigates LLM challenges such as
hallucinations [30] and outdated knowledge [31].

Several studies have advanced the application of RAG in
code generation. REDCODER [32] retrieves relevant code
snippets from a corpus to enrich the context for code genera-
tion, while ReACC [33] combines token-level and semantic
similarity metrics to extract pertinent code. CEDAR [34]
achieves state-of-the-art performance in assertion generation
and program repair by leveraging embedding-based retrieval
and frequency analysis. DocPrompting [35] emulates program-
mers’ use of documentation by querying relevant fragments to
assist in target code generation.

Despite these advancements, RAG-based methods remain
highly dependent on retrieval quality; irrelevant, erroneous,
or redundant content can significantly degrade the quality
of generated code. Furthermore, existing approaches often
lack effective mechanisms to bridge the gap between retrieval
and generation, resulting in outputs that may not fully align
with user requirements. To address these limitations, DLCode-
Gen introduces dual retrieval-augmented strategies: retrieving
similar code for concrete implementation references and ab-
stracting templates to enhance generalization. A comparative
learning mechanism synergizes the strengths of both strategies,
while deep learning solution plans ensure semantic coherence
and consistency throughout the generation process.

A. Overview

III. APPROACH

Given a natural language requirement X = x1, x2, . . . , xm,
the objective of our approach is to generate the corresponding
source code for a deep learning project C = c1, c2, . . . , cn.

To address the challenges posed by the inherent complexity
of deep learning projects, we propose DLCodeGen, a novel
planning-guided generation method. DLCodeGen employs a
fine-tuned model to predict a structured solution plan, offering
global guidance for LLMs to generate comprehensive and
coherent deep learning projects.
As illustrated in Figure 1,

the pipeline of DLCodeGen
consists of three key stages. First, a GPT-2 model is fine-
tuned to predict a solution plan for a deep learning project
based on a given natural language requirement. Second, the
generated plan is leveraged to retrieve semantically analogous
code samples (Code RAG) and subsequently abstract a code
template (Template RAG) by utilizing an LLM. Third, a com-
parative learning mechanism is designed to synthesize the final
code through the integration of Code RAG and Template RAG,
which serve as the references for fine-grained implementation
details and high-level architectural logic, respectively. Each
step is elaborated in detail in the following subsections.

B. Solution Plan Prediction

Unlike general-purpose code, deep learning projects often
adhere to consistent architectural patterns, enabling the use
of structured solution plans to guide LLMs. A solution plan
encapsulates the essential components and standardized work-
flows intrinsic to deep learning projects, encompassing the
entire pipeline from data preprocessing to model evaluation.
A solution plan typically consists of the following four core

components:

1) Task Category: Identifies the primary type of machine
learning task, such as Image Classification, Text Gener-
ation, or Object Detection.

2) Dataset: Describes the specifications of both input and
output data, including their dimensionalities, formats,
and other relevant attributes.

3) Preprocess: Outlines the sequence of transformations
necessary to convert raw data into a format suitable for
model input, including operations such as normalization,
feature extraction, and the partitioning of datasets into
training, validation, and test sets.

1.Solution Plan PredictionSolutionPlan2.Code Retrieval & AbstractingSample Pool3.Code GenerationTarget CodeSolution PlanPredictionGPT2TemplateAbstractionCodeRetrievalSimilar CodeTemplate Code RAGTemplate RAGComparative GenerationFig. 3: An Illustration of Comparative Generation

relevant code samples are identified through similarity match-
ing between solution plans using the BM25 [36] algorithm.

More specifically,

the code sample pool

is organized
into a collection of task-specific subsets, formally repre-
sented as B = R1, R2, . . . , Rl, where each subset Ri =
{(Si1, Ci1), (Si2, Ci2), . . . } consists of multiple solution-code
pairs (Si, Ci), each corresponding to a distinct category of
deep learning tasks. Given a newly generated solution plan
Snew derived from a user requirement, we first identify the
corresponding subset R according to its task category. We then
compute the similarity scores between S and each plan Sj in
Rt using the BM25 algorithm. The top-ranked code samples,
as determined by their similarity scores, are selected to inform
the subsequent code generation process.

Template RAG: The implementation details present in the
retrieved code samples may introduce noise that interferes
with the generative capabilities of LLMs. To mitigate this
issue, DLCodeGen synthesizes the retrieved code samples
into a higher-level code template, which provides a logi-
cally coherent and domain-agnostic framework for subsequent
code generation. The abstraction process effectively eliminates
implementation-level variations while retaining the essential
structural and functional patterns necessary for the target task.
Specifically, we utilize an LLM to extract recurring struc-
tural patterns from the top two retrieved code samples, en-
compassing data preprocessing workflows, model architecture
definitions, and training pipeline configurations. Furthermore,
we generalize the core interfaces and logical frameworks
of each functional module, removing implementation-specific
details such as hardcoded data formats or task-dependent con-
figurations. These modularized frameworks are then integrated
into a unified template that demonstrates high transferability
and reusability across a wide range of deep learning tasks.

D. Code Generation

Guided by the solution plan, DLCodeGen utilizes a com-
parative learning mechanism to integrate the outputs of Code
RAG and Template RAG, directing LLMs to generate the
final code, as illustrated in Figure 3. In this framework, the
solution plan provides global guidance to ensure contextual
consistency, while Template RAG offers high-level structural

Fig. 2: An example of a solution plan for a deep learning project

4) Model Architecture: Specifies the structural configura-
tion of the model, including layer compositions, connec-
tivity patterns, and the initialization and optimization of
hyperparameters.

Figure 2 presents an illustrative example of a solution plan
for an image classification task. Initially, all input images are
normalized to ensure a standardized data distribution, followed
by dataset partitioning facilitated by the ImageDataGenerator
utility. The proposed model architecture consists of six layers,
optimized using the Adam optimization algorithm in con-
junction with the cross-entropy loss function to achieve effec-
tive parameter tuning.

Given natural language requirements as input, our objective
is to predict a comprehensive solution plan that functions as
a structured blueprint to guide subsequent code generation.
To formalize this problem, we frame it as a sequence-to-
sequence learning task. We address this task by fine-tuning a
pre-trained GPT-2 model, capitalizing on its robust language
understanding capabilities to generate precise and contextually
relevant solution plans.

C. Similar Code Retrieval and Abstracting

The generated solution plans play a pivotal role in guiding
LLMs to generate deep learning projects. To maximize their
utility, we employ these plans as foundational prompts and
further enrich the LLM’s input through the retrieval of similar
code and the abstraction of high-level templates, i.e., Code
RAG and Template RAG.

Code RAG: DLCodeGen retrieves semantically relevant
code from a pre-built code sample pool using the generated
solution plan as a query. The retrieval process is conducted in
two stages: first, the search scope is narrowed based on the
task type specified in the solution plan; second, semantically

{"User Requirement": "Build a model to classify images as violence or non-violence using deep learning.","Dataset Attributes": "Dataset includes RGB images (224x224 pixels) labeled as 'violence' or 'non-violence', with several thousand instances.","Code Plan": {"Task Category": "Image Classification","Dataset": {"Input": "(batch_size, 224, 224, 3), images resized to 224x224 pixels with 3 color channels.","Output": "(batch_size, 2), representing 'violence' and 'non-violence' classes."},"Preprocess": "Images rescaled to [0, 1] using ImageDataGeneratorand split into train/val/test sets.","Model Architecture": {"Layers": ["Input(shape=(224, 224, 3))","InceptionV3(include_top=False, weights='imagenet')","Flatten()", "Reshape((8, -1))","LSTM(64, return_sequences=True)","Dense(128, activation='relu')","Dense(2, activation='softmax')"],"Hyperparameters": {"optimizer": "adam","loss function": "categorical_crossentropy","learning rate": 0.001,"batch size": 32,"epochs": 20,"evaluation metric": "accuracy"}}}}Requirement: image cls…Task: … Input: …Predicted Solution# Data processx,x_test, y, y_test = ..# Modelmodel = Sequential[…])…DL Code 1# DL Code 1# DL Code 2# solutionComparison Prompt# Data processx,x_test, y, y_test = ..# Modelmodel = Sequential[…])…DL Code 2# Data processx,x_test, y, y_test = ..# Modelmodel = Sequential([Conv1D(…)])model.compile(Adam(lr), SparseCateCE(), ‘accuracy’)model.fit(x,y,batch_size)# Evaluationacc = model.evaluate(x_test, y_test)Generated DL Codeframeworks and Code RAG delivers implementation-specific
details.

Specifically, DLCodeGen first prompts LLMs with similar
code and abstracted templates to generate two code outputs,
referred to as code 1 and code 2, each featuring unique
strengths and limitations. code 1, generated via Code RAG,
incorporates task-specific details and contextual information,
though it may include extraneous elements unrelated to the
target task, potentially compromising output quality. In con-
trast, code 2, produced through Template RAG, adopts a
broader perspective with a standardized framework, emphasiz-
ing structural coherence over granular details, which may lead
to implementation deviations due to insufficient specificity.

To harness the complementary advantages of both RAG
strategies, DLCodeGen implements a comparative learning
mechanism. This involves constructing comparative prompts
by integrating the solution plan with dual RAG-generated
code, formatted as <solution, code 1, code 2>. The LLM is
guided to compare the two provided deep learning code out-
puts by utilizing the comprehensive insights from the solution
plan, and subsequently identifies the optimal segments from
each to inform the synthesis of the final deep learning project.
By employing comparative learning, DLCodeGen not only
corrects potential errors but also stimulates deeper reasoning,
fostering greater procedural knowledge and flexibility [37].
This enables the LLM to effectively integrate knowledge from
diverse sources, resulting in deep learning code that is more
robust, logically consistent, and comprehensive.

IV. EXPERIMENTAL SETUP

A. Research Questions

We evaluate DLCodeGen by addressing the following re-

search questions.

• RQ1: How effective is DLCodeGen in deep learn-
ing code generation? We assess the performance of
DLCodeGen in generating deep learning projects and
compare it against state-of-the-art baseline methods.

• RQ2: To what extent

is the deep learning code
generated by DLCodeGen compliant, practical, and
idiomatic? In addition to automated evaluation metrics,
we conduct a human study to evaluate the intrinsic quality
of the generated code, focusing on its practicality for
developers and adherence to programming conventions.
• RQ3: How accurate is the plan prediction model in
DLCodeGen? We construct a benchmark for evaluat-
ing the prediction of deep learning solution plans and
compare the quality of plans generated by DLCodeGen
against those produced directly by LLMs.

• RQ4: What is the contribution of each component in
DLCodeGen? We perform an ablation study to analyze
the impact of the three core components of DLCodeGen:
Code RAG, Template RAG, and the comparative learning
mechanism.

• RQ5: How does the temperature setting in LLMs
influence the performance of DLCodeGen? Given the

sensitivity of LLM-generated code quality to temperature
settings, we investigate the effect of varying temperature
values on the overall performance of DLCodeGen.

B. Comparison Methods

We compare DLCodeGen with existing methods for deep
tech-
learning code generation. Due to the absence of
niques specifically designed for deep learning code genera-
tion, we adapt state-of-the-art general-purpose code genera-
tion approaches, including planning-guided methods, chain-
of-thought prompting, and retrieval-augmented generation, to
the task of deep learning project generation. Additionally, we
include direct LLM generation as a baseline for comparative
analysis. Overall, we evaluate DLCodeGen against the follow-
ing baseline methods:

• Direct: A baseline approach where deep learning code is
generated directly by LLMs based on user-provided re-
quirements without additional guidance or augmentation.
• PS (Plan-and-Solve Prompting) [27]: A planning-guided
approach that instructs the LLM to explicitly formulate a
solution plan before generating code, producing interme-
diate reasoning steps to guide the final output.

• C-CoT (Cluster-CoT) [15]: A chain-of-thought (CoT)
prompting approach that selects representative problems
from each task category and employs heuristic methods
combined with zero-shot CoT to generate task-specific
the model
reasoning chains. During code generation,
utilizes the relevant CoT prompt corresponding to the task
category.

• CEDAR (Code Example Demonstration Automated Re-
trieval) [34]: A retrieval-augmented approach that extracts
task-relevant code examples from a codebase to construct
contextually enriched prompts for code generation.

We replicate the results of PS, C-CoT, and CEDAR using
their officially released codebases. To adapt PS for deep
learning code generation, we modify its implementation to
generate intermediate reasoning steps aligned with the deep
learning solution framework we designed, followed by the
generation of deep learning code. For CEDAR, we employ
a sparse retrieval mechanism to extract the most relevant code
examples from the codebase based on user requirements. To
ensure fairness and consistency with our approach, the number
of retrieved code examples is limited to one.

C. Evaluation Metrics

We use two kinds of metrics to evaluate the quality of the

generated deep learning code:
CodeBLEU [38]: CodeBLEU is a widely used metric specif-
ically designed for code generation tasks. In addition to
weighted n-gram matching, it also incorporates syntax tree
matching and semantic data flow matching.
Human Evaluation Metrics: To assess the intrinsic quality of
the generated code, we define the following human evaluation
metrics:

• Compliance: Evaluates the extent to which the generated
code fulfills the functional requirements specified in the
user request.

• Helpfulness: Assesses the degree to which the generated
code assists users in understanding and incrementally
developing their complete project.

• Idiomaticity: Measures the adherence of the generated
code to programming conventions, including readability,
structure, and best practices.

All metrics are scaled from 0 to 10, with higher scores

indicating superior performance.

We assess the performance of the plan prediction model
using the following metrics, encompassing both overall and
field-specific evaluations:
BLEU [39]: BLEU is a widely adopted metric for assessing
the quality of machine translation, measuring the n-gram over-
lap between generated and reference text. In our experiments,
we employ BLEU-4, which averages precision scores from
1-gram to 4-gram, to evaluate both the overall quality of
predicted plans and Input & Output fields.
EM (Exact Match) [40]: EM measures the exact correspon-
dence of the Task Category, Loss Function, and Optimizer
fields in the predicted plans.
MLD (Mean Levenshtein Distance) [41]: MLD computes the
average Levenshtein distance among elements within an array,
serving to evaluate the Layers field.
MAE (Mean Absolute Error) [42]: MAE calculates the aver-
age absolute difference between predicted and actual values,
used to evaluate numerical accuracy in the Hyperparameters
field.

D. Datasets

We construct the parallel corpus required for our experi-
ments using the Meta Kaggle Code [43] dataset, which con-
sists of hundreds of thousands of publicly accessible Python
and R language notebooks sourced from the Kaggle platform.
Initially, we filter Jupyter Notebook files from the Meta
Kaggle Code dataset, identified by the .ipynb extension. These
files include complete code execution workflows, annotations,
and output
information. Subsequently, we apply keyword-
based filtering using terms relevant to the TensorFlow frame-
work to exclude notebooks employing traditional machine
learning methods, retaining only those utilizing the Tensor-
Flow and Keras frameworks. Duplicate entries are removed
based on the content of the Code Cells. This selection process
results in a final collection of 3,950 deep learning notebooks.
For these refined notebooks, we employ an LLM to extract
and summarize corresponding user requirements and solution
plans. As a result, we construct the following three datasets:
is designed to evaluate the quality of
generated deep learning code. To prevent data leakage, we
select notebook files with execution timestamps after 2024 and
verify through LLMs that these files have not been exposed
in their training data.

1) DLCodeEval

TABLE I: Dataset Statistics

Dataset

Data Size

Data Content

100/10a
40

DLCodeEval
DLSamplePool
DLPlanData
aAutomated Testing Set / Manual Evaluation Set.
bTrain / Evaluation / Test.

<user requirement, code>
<code, solution>

3300/100/400b <user requirement, solution>

2) DLSamplePool provides a curated collection of high-
quality deep learning code samples. Each sample undergoes
manual review to ensure its relevance and quality.

3) DLPlanData comprises the remaining files, which are
utilized as solution plan data for training, evaluating, and
testing the solution plan predictor.

The statistics of these datasets are presented in Table I.
Notably, the average lines of code in these datasets is 331,
significantly exceeding the 20-line average in the widely used
HumanEval-X benchmark for general-purpose code genera-
tion. This highlights the complexity and scale of deep learning
projects compared to general-purpose coding tasks.

E. Implementation Details

We select DeepSeek-V2.51 as the base model for code

generation.

For the plan predictor, we employ GPT-2-large 2 as the
foundational model and fine-tune it on our DLPlanData. The
model is initialized with the open-source weights provided
by HuggingFace, utilizing the default BPE tokenizer and
adhering to standard text
length limitations. We optimize
the model using the AdamW optimizer with a learning rate
of 1e-3, a batch size of 16, and train it for 8 epochs. To
determine the optimal decoding strategy, we evaluate two
approaches: greedy search and beam search. While greedy
search offers faster inference, it often generates less coherent
text in complex tasks. To achieve higher-quality and more
coherent outputs while balancing computational complexity,
DLCodeGen adopts beam search as the decoding strategy. To
optimize the trade-off between search space and computational
efficiency, we set the num_beams parameter to 3.

A. Effectiveness of DLCodeGen (RQ1)

V. RESULTS

We compare DLCodeGen against baseline methods on the
DLCodeEval using CodeBLEU and its four sub-metrics. To
understand the transferability of our approach, we also conduct
experiments on GPT-4o-mini, a proprietary state-of-the-art
LLM. As shown in Table II, DLCodeGen consistently outper-
forms the baseline methods across both backbone language
models, achieving the highest overall CodeBLEU score.

Specifically, when evaluated on the DeepSeek-V2.5 model,
DLCodeGen achieves a significant improvement over the best-
performing baseline, CEDAR, with a 2.34-point increase in

1https://huggingface.co/deepseek-ai/DeepSeek-V2.5
2https://huggingface.co/openai-community/gpt2-large

TABLE II: Automated Evaluation of Various Approaches in Deep Learning Code Generation

Model

Method

ngram w-ngram syntax-match

dataflow-match

CodeBLEU

DeepSeek-V2.5

GPT-4o-mini

Direct
PS
C-CoT
CEDAR
DLCodeGen (ours)

Direct
PS
C-CoT
CEDAR
DLCodeGen (ours)

4.23
4.57
6.33
10.71
12.81

3.02
3.05
3.52
8.38
9.78

10.61
13.31
14.32
15.07
18.55

9.80
11.59
11.44
13.85
16.15

47.62
49.95
52.05
54.94
56.27

49.39
49.94
51.42
54.37
54.81

13.43
13.70
18.18
20.01
22.44

12.71
12.61
16.78
19.30
22.12

18.97
20.38
22.72
25.18
27.52

18.73
19.30
20.79
23.97
25.71

overall performance, representing a 9.3% enhancement. Fur-
thermore, DLCodeGen surpasses competing methods across
all four sub-metrics, demonstrating notable improvements in
n-gram, w-ngram, syntax-match, and dataflow-match scores
by 19.6%, 23.1%, 2.4%, and 12.1%, respectively.

A similar trend is observed on the GPT-4o-mini model,
where DLCodeGen achieves a 7.3% increase in CodeBLEU
over CEDAR. The improvements across individual sub-metrics
are as follows: n-gram (16.7%), w-ngram (16.6%), syntax-
match (0.8%), and dataflow-match (14.6%). Notably,
the
the most
dataflow-match and w-ngram sub-metrics exhibit
significant absolute improvements, with gains of 2.82 and 2.3
points, respectively.

These results indicate that the deep learning code gener-
ated by DLCodeGen not only exhibits higher textual pattern
similarity but also aligns more accurately with the semantic
and data flow structures of the reference code, highlighting its
superior quality and robustness.

Answer to RQ1: DLCodeGen consistently outperforms state-
of-the-art methods in deep learning project generation across
all backbone LLMs, as demonstrated by higher scores in both
the overall CodeBLEU metric and its four sub-metrics.

B. Qualitative Evaluation (RQ2)

To evaluate the intrinsic quality of the generated deep learn-
ing projects, we perform a human study involving experienced
programmers. Four participants from author’s institution, but
affiliated with different labs, are recruited through targeted
invitations. All participants are postgraduate researchers spe-
cializing in artificial intelligence or software engineering, with
over three years of programming experience in deep learning
projects. They independently assess the generated code and
provide detailed justifications for their ratings. In cases of
significant score discrepancies, a collaborative discussion is
facilitated to resolve differences and achieve a consensus
among all annotators.

Table III presents the results of the human evaluation, as
scored by the annotators. Overall, DLCodeGen achieves the
highest average scores across all metrics. Specifically, in terms
of both compliance and idiomaticity, DLCodeGen attains the
top scores of 8.30 and 7.50, outperforming the strong baseline,
CEDAR, by 7.8% and 5.6%, respectively. This indicates that

TABLE III: Human Evaluation Results

Method

Compliance

Usefulness

Idiomaticity

Average

Direct
PS
C-CoT
CEDAR
DLCodeGen

7.40
7.80
7.90
7.70
8.30

5.40
6.00
6.20
6.40
6.20

6.50
6.60
6.90
7.10
7.50

6.43
6.80
7.00
7.07
7.33

our approach excels in generating code that not only fulfills
user-specified functional requirements but also adheres more
closely to programming conventions. This improvement can be
attributed to the guidance provided by the predicted solution
plans and the augmentation of abstract code templates.

In terms of usefulness, DLCodeGen achieves a score of
6.20, marginally lower than CEDAR’s score of 6.40. This mi-
nor discrepancy may arise from CEDAR’s direct incorporation
of code samples retrieved from real-world projects. While its
generated outputs may not always precisely align with specific
functional requirements, they often provide adaptable start-
ing points, underscoring the inherent advantage of retrieval-
augmented techniques in practical code generation scenarios.

Answer to RQ2: DLCodeGen demonstrates significant im-
provements of 7.8% and 5.6% in compliance and idiomatic-
ity, respectively, while achieving comparable performance in
helpfulness when compared to the strong baseline, CEDAR.

C. Performance of Plan Predictor (RQ3)

Given the pivotal role of the plan predictor in our approach,
we evaluate its performance on the DLPlanData using two
decoding strategies: beam search and greedy search. For
comparative analysis, we select few-shot learning-based GPT-
3.5-turbo and GPT-4o-mini as baseline methods.

The results, as illustrated in Figure 4, demonstrate that the
fine-tuned GPT-2-large model with beam search achieves the
highest performance, highlighting the efficacy of our approach.
While GPT models exhibit strong code generation capa-
bilities (particularly GPT-4o-mini, whose 3-shot performance
surpasses that of the fine-tuned GPT-2-large model using
greedy search), general-purpose LLMs still underperform in
specialized deep learning code generation tasks. This limita-
tion stems from the absence of domain-specific training and

RAG, thereby significantly enhancing the overall performance
of DLCodeGen.

The results further reveal that removing Code RAG and
Template RAG leads to score reductions of 1.06 and 0.69,
respectively, highlighting the individual contributions of both
components to the final performance. Notably, when the out-
puts of these two components are replaced with those from C-
CoT and CEDAR, the score drops significantly to 25.63, which
is only marginally higher than the scores of variants without
the comparative learning mechanism. This is attributed to the
absence of global contextual information in the code generated
by C-CoT and CEDAR, which limits the comparative learning
mechanism’s ability to achieve a complementary effect.

In summary, our approach employs a comparative learning
mechanism to effectively integrate knowledge from Code RAG
and Template RAG, collectively producing deep learning code
that is more robust, logically consistent, and comprehensive.
Answer to RQ4: Code RAG, Template RAG, and comparative
learning synergistically contribute to the enhanced effective-
ness of our generation technique for deep learning projects.
Code RAG and Template RAG provide complementary im-
plementation and structural
insights, which are seamlessly
integrated by the comparative learning mechanism to achieve
robust and high-quality code generation.

E. Temperature Setting (RQ5)

We evaluate DLCodeGen across a range of temperature
settings (0.0, 0.5, 1.0, 1.5, 2.0) to investigate the impact of
temperature on generation quality and determine the optimal
parameter configuration.

As shown in Figure 5, the overall CodeBLEU score initially
increases and then declines as the temperature rises, peaking at
27.96 with a temperature setting of 1.5. At this optimal point,
the sub-metrics syntax-match and dataflow-match also reach
their highest values, while ngram and w-ngram scores remain
relatively lower. Considering that semantic alignment is more
critical than textual similarity in code evaluation, we identify
1.5 as the optimal temperature configuration for DLCodeGen.
It is noteworthy that this value is significantly higher than
the temperature settings typically used by other baseline
methods, which are often set to 0. We attribute this to the
fact that a higher temperature encourages greater exploration
of potential generation paths. Simultaneously, DLCodeGen’s
comparative learning mechanism ensures the quality of the
generated code, effectively mitigating the risk of low-quality
outputs that are commonly associated with higher temperature
settings.
Answer to RQ5: The temperature setting significantly influ-
ences the quality of code generated by DLCodeGen. Using
DeepSeek-V2.5 as the base model, a temperature configuration
of 1.5 yields the optimal results.

A. Why is DLCodeGen Effective?

VI. DISCUSSION

We attribute the effectiveness of our DLCodeGen to the

following three factors:

Fig. 4: Performance of Solution Plan Predictor

fine-tuning, emphasizing the necessity of tailored optimization
for domain-specific applications to achieve optimal results.

To further examine the performance differences among
various methods in predicting deep learning plans, we perform
a field-level analysis of the generated plans. The results are
summarized in Table IV. The fine-tuned GPT-2-large model
achieves superior performance across most metrics, achiev-
ing the lowest error rate of 28.39 on the critical Layers
field, significantly outperforming the two baseline LLMs. This
substantial performance gap underscores the fine-tuned plan
predictor’s distinct advantage in accurately predicting deep
learning architecture layers, a task that demands extensive
domain-specific expertise.

Notably, GPT-4o-mini demonstrates exceptional perfor-
mance in the Input and Output fields, both of which involve
extensive textual descriptions. This indicates that the large-
scale parameter architecture of general-purpose LLMs pro-
vides a significant advantage in handling complex text gen-
eration tasks. Conversely, the fine-tuned GPT-2-large model
exhibits potential for further improvement in predicting the
Loss Function and Optimizer fields, suggesting opportunities
for enhancing domain-specific fine-tuning.

Answer to RQ3: The fine-tuned GPT-2-large model surpasses
general-purpose LLMs in solution plan prediction, delivering
superior performance in both comprehensive and field-level
evaluations.

D. Contribution of Components (RQ4)

To examine the contribution of the three core components,
Code RAG, Template RAG, and comparative learning, to the
performance of our approach, we conduct an ablation study.
Specifically, we remove one or two components at a time and
evaluate the performance of the resulting ablated models.

The results are summarized in Table V. A significant decline
in CodeBLEU scores is observed across all ablated variants.
Notably, variants lacking the comparative learning mechanism
exhibit the most substantial performance degradation, with
score drops of 1.95 and 2.19, respectively. This underscores
the critical role of the comparative learning mechanism in ef-
fectively integrating the strengths of Code RAG and Template

TABLE IV: Field-level Performance of Plan Predictor

Model

Category↑

Input↑ Output↑

Layers↓

LR↓

LF↑

Optimizer↑

BS↓

Epoch↓ Metric↑

GPT-3.5-turbo(3-shot)
83.50%
GPT-3.5-turbo(5-shot)
81.25%
GPT-4o-mini(3-shot)
88.00%
GPT-4o-mini(5-shot)
89.50%
GPT-2-large(greedy)
86.75%
91.75%
GPT-2-large(beam)
∗LR = Learning Rate; LF = Loss Function; BS = Batch size.

26.71
24.48
32.97
32.12
28.51
32.04

23.18
20.85
27.88
27.12
26.73
26.90

35.00
33.44
35.62
37.42
28.66
28.39

0.20
0.20
0.20
0.20
0.12
0.09

40.25%
34.25%
74.75%
77.25%
62.25%
67.00%

64.00%
63.75%
64.25%
63.50%
63.25%
62.75%

132.99
134.77
133.04
134.00
123.11
128.26

71.40
72.50
70.58
71.76
80.70
69.14

53.75%
50.25%
56.75%
58.50%
65.75%
66.50%

TABLE V: Component Ablation Study

Method

ngram w-ngram syn-match

df-match

CodeBLEU

DLCodeGen
- w/o Comparison & CR
- w/o Comparison & TR
- w/o CR
- w/o TR
- w/o CR & TR
∗ CR = Code RAG; TR = Template RAG.

12.81
10.70
9.98
12.38
12.47
12.28

18.55
17.34
16.45
17.29
17.75
16.55

56.27
55.21
54.68
54.15
55.12
53.90

22.44
19.05
20.19
22.01
22.00
19.77

27.52
25.57
25.33
26.46
26.83
25.63

(a) CodeBLEU

(b) ngram

(c) w-ngram

(d) syntax-match

(e) dataflow-match

Fig. 5: The Trend of Scores with Temperature Changes

1) Planning-Guided Generation: The solution plan pre-
dictor, fine-tuned on a high-quality parallel corpus, provides
domain-specific knowledge and ensures global consistency
throughout the code generation process.

2) Abstract Template Augmentation: The abstraction of gen-

eral templates mitigates the sensitivity of RAG to the quality
of retrieved code samples, thereby enhancing the robustness
and reliability of the generated code.

3) Comparative Learning Mechanism: This mechanism
effectively integrates the strengths of dual RAG strategies,
enabling error correction while stimulating deeper reasoning.
This fosters greater procedural knowledge and flexibility in
the generated code.

B. Limitation and Threats to Validity

We have identified the following limitations and potential

threats to the validity of our method:

Data Leakage: DLCodeGen relies on LLMs to generate
deep learning code. As the scale of these models increases,
so does the scope of their training data, raising the possibility
of exposure to data from our test dataset. To mitigate this
risk, we have implemented time-based filtering and knowledge
detection strategies to minimize overlap between the test
dataset and the training data. However, a residual risk of data
leakage remains despite these precautions.

Evaluation of Generated Code: The effectiveness of DL-
CodeGen is assessed using metrics such as CodeBLEU,
Compliance, Helpfulness, and Idiomaticity. However, another
widely recognized metric for evaluating code generation tasks
is the test pass rate. Future work will involve developing a test-
based benchmark to provide a more comprehensive evaluation
of our approach’s performance on real-world tasks.

VII. CONCLUSION

In this paper, we present DLCodeGen, a novel planning-
guided method for deep learning code generation. DLCodeGen
fine-tunes a model on a carefully curated parallel corpus to
generate solution plans that precisely align with user require-
ments. These plans are are utilized to retrieve semantically
analogous code samples and derive a code template. Further-
more, DLCodeGen employs a comparative learning mecha-
nism to integrate outputs from Code RAG and Template RAG,
significantly improving the quality of the final code. We vali-
date the effectiveness of DLCodeGen through both automatic
and human evaluations. Experimental results demonstrate that
DLCodeGen produces highly accurate and idiomatic code for
deep learning projects, outperforming baseline methods by a
substantial margin.

0.00.51.01.52.027.427.527.627.727.827.928.028.10.00.51.01.52.013.013.113.213.313.40.00.51.01.52.018.818.818.818.918.918.918.90.00.51.01.52.055.255.455.655.856.056.256.40.00.51.01.52.022.022.222.422.622.823.023.223.423.6DATA AVAILABILITY

Our source code and experimental data for replication
are publicly available at: https://github.com/MingshengJiao/
DLCodeGen.

ACKNOWLEDGMENTS

This research is supported by National Key R&D Pro-
gram of China (Grant No. 2023YFB4503802) and National
Natural Science Foundation of China (Grant No. 62102244,
62232003).

REFERENCES

[1] T. Ben-Nun and T. Hoefler, “Demystifying parallel and distributed
deep learning: An in-depth concurrency analysis,” ACM Comput. Surv.,
vol. 52, no. 4, pp. 65:1–65:43, 2019.

[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
L. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS,
2017, pp. 5998–6008.

[3] A. Voulodimos, N. Doulamis, A. D. Doulamis, and E. Protopapadakis,
“Deep learning for computer vision: A brief review,” Comput. Intell.
Neurosci., pp. 7 068 349:1–7 068 349:13, 2018.

[4] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in
deep learning based natural language processing,” IEEE Comput. Intell.
Mag., vol. 13, no. 3, pp. 55–75, 2018.

[5] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo,
J. Grundy, and H. Wang, “Large language models for software engineer-
ing: A systematic literature review,” ACM Trans. Softw. Eng. Methodol.,
vol. 33, no. 8, pp. 220:1–220:79, 2024.

[6] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond,
T. Eccles, J. Keeling et al., “Competition-level code generation with
alphacode,” Science, vol. 378, no. 6624, pp. 1092–1097, 2022.

[7] F. Liu, G. Li, Y. Zhao, and Z. Jin, “Multi-task learning based pre-trained
language model for code completion,” in ASE, 2020, pp. 473–485.
[8] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “Intellicode
compose: code generation using transformer,” in ESEC/FSE, 2020, pp.
1433–1443.

[9] J. Liu, C. S. Xia, Y. Wang, and L. Zhang, “Is your code generated by
chatgpt really correct? rigorous evaluation of large language models for
code generation,” in NeurIPS, 2023.

[10] Y. Dong, X. Jiang, Z. Jin, and G. Li, “Self-collaboration code generation
via chatgpt,” ACM Trans. Softw. Eng. Methodol., vol. 33, no. 7, 2024.
[11] H. Yu, B. Shen, D. Ran, J. Zhang, Q. Zhang, Y. Ma, G. Liang, Y. Li,
Q. Wang, and T. Xie, “Codereval: A benchmark of pragmatic code
generation with generative pre-trained models,” in ICSE, 2024, pp. 37:1–
37:12.

[12] M. Li and B. Krishnamachari, “Evaluating chatgpt-3.5 efficiency in
solving coding problems of different complexity levels: An empirical
analysis,” CoRR, vol. abs/2411.07529, 2024.

[13] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi,
Q. V. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning
in large language models,” in NeurIPS, 2022.

[14] J. Li, G. Li, Y. Li, and Z. Jin, “Structured chain-of-thought prompting
for code generation,” ACM Trans. Softw. Eng. Methodol., 2024.
[15] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought

prompting in large language models,” in ICLR, 2023.

[16] X. Jiang, Y. Dong, L. Wang, Z. Fang, Q. Shang, G. Li, Z. Jin, and
W. Jiao, “Self-planning code generation with large language models,”
ACM Transactions on Software Engineering and Methodology, vol. 33,
no. 7, pp. 1–30, 2024.

[17] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,
H. K¨uttler, M. Lewis, W. Yih, T. Rockt¨aschel, S. Riedel, and D. Kiela,
“Retrieval-augmented generation for knowledge-intensive NLP tasks,”
in NeurIPS, 2020.

[18] M. A. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten, “The WEKA data mining software: an update,” SIGKDD
Explor., vol. 11, no. 1, pp. 10–18, 2009.

[19] Y. Wang, W. Song, Y. Yang, C. Mahmoudi, S. Shekhar, and K. P.
Birman, “Dash: A low code development platform for AI applications
in industry,” in UEMCON, 2023, pp. 72–81.
[20] R. Leenings, N. R. Winter, K. Sarink,

Jiang,
U. Dannlowski, and T. Hahn, “The PHOTON wizard - towards educa-
tional machine learning code generators,” CoRR, vol. abs/2002.05432,
2020.

J. Ernsting, X.

[21] N. Rao, J. Tsay, K. Kate, V. J. Hellendoorn, and M. Hirzel, “AI for

low-code for AI,” in IUI, 2024, pp. 837–852.

[22] J. Xu, J. Li, Z. Liu, N. A. V. Suryanarayanan, G. Zhou, J. Guo, H. Iba,
and K. Tei, “Large language models synergize with automated machine
learning,” CoRR, vol. abs/2405.03727, 2024.

[23] Y. Gu, H. You, J. Cao, and M. Yu, “Large language models for
constructing and optimizing machine learning workflows: A survey,”
CoRR, vol. abs/2411.10478, 2024.

[24] S. Zhang, C. Gong, L. Wu, X. Liu, and M. Zhou, “Automl-gpt:
Automatic machine learning with GPT,” CoRR, vol. abs/2305.02499,
2023.

[25] L. Zhang, Y. Zhang, K. Ren, D. Li, and Y. Yang, “Mlcopilot: Unleashing
the power of large language models in solving machine learning tasks,”
in EACL, 2024, pp. 2931–2959.

[26] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt:
Solving AI tasks with chatgpt and its friends in hugging face,” in
NeurIPS, 2023.

[27] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K. Lee, and E. Lim, “Plan-
and-solve prompting: Improving zero-shot chain-of-thought reasoning
by large language models,” in ACL, 2023, pp. 2609–2634.

[28] L. Zhong, Z. Wang, and J. Shang, “Debug like a human: A large
language model debugger via verifying runtime execution step by step,”
in Findings of ACL, 2024, pp. 851–870.

[29] S. Jiang, Y. Wang, and Y. Wang, “Selfevolve: A code evolution frame-

work via large language models,” CoRR, vol. abs/2306.02907, 2023.

[30] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu et al., “Siren’s song in the
AI ocean: A survey on hallucination in large language models,” CoRR,
vol. abs/2309.01219, 2023.

[31] J. Jang, S. Ye, S. Yang, J. Shin, J. Han, G. Kim, S. J. Choi, and M. Seo,
“Towards continual knowledge learning of language models,” in ICLR,
2022.

[32] M. R. Parvez, W. U. Ahmad, S. Chakraborty, B. Ray, and K. Chang,
“Retrieval augmented code generation and summarization,” in Findings
of EMNLP, 2021, pp. 2719–2734.

[33] S. Lu, N. Duan, H. Han, D. Guo, S. Hwang, and A. Svyatkovskiy,
“Reacc: A retrieval-augmented code completion framework,” in ACL,
2022, pp. 6227–6240.

[34] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt selec-
tion for code-related few-shot learning,” in ICSE, 2023, pp. 2450–2462.
[35] S. Zhou, U. Alon, F. F. Xu, Z. Jiang, and G. Neubig, “Docprompting:

Generating code by retrieving the docs,” in ICLR, 2023.

[36] S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, M. Gat-
ford et al., “Okapi at trec-3,” Nist Special Publication Sp, vol. 109, p.
109, 1995.

[37] T. Lombrozo, “Learning by thinking in natural and artificial minds,”

Trends in Cognitive Sciences, vol. 28, p. 1, 2024.

[38] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan,
M. Zhou, A. Blanco, and S. Ma, “Codebleu: a method for automatic
evaluation of code synthesis,” CoRR, vol. abs/2009.10297, 2020.
[39] K. Papineni, S. Roukos, T. Ward, and W. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in ACL, 2002, pp. 311–
318.

[40] S. Zhang, X. Gu, Y. Chen, and B. Shen, “Infere: Step-by-step regex
IEEE, 2023, pp. 1505–

generation via chain of inference,” in ASE.
1515.

[41] F. Petersen, M. Schubotz, A. Greiner-Petter, and B. Gipp, “Neu-
ral machine translation for mathematical formulae,” arXiv preprint
arXiv:2305.16433, 2023.

[42] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,
“Informer: Beyond efficient transformer for long sequence time-series
forecasting,” in AAAI, vol. 35, no. 12, 2021, pp. 11 106–11 115.
[43] J. Plotts and M. Risdal, “Meta kaggle code,” 2023. [Online]. Available:

https://www.kaggle.com/ds/3240808


